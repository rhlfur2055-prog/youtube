"""
============================================================================
 youshorts ì™„ë²½í•œ ì˜ìƒ 1ê°œ ìƒì„± ë§ˆìŠ¤í„° íŒŒì´í”„ë¼ì¸
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 ì—­í• : í’€ìŠ¤íƒ ê°œë°œì + ìˆì¸  ë°°í¬ ì „ë¬¸ê°€
 ëª©í‘œ: íŠ¸ë Œë“œ ìˆ˜ì§‘ â†’ ëŒ€ë³¸ â†’ TTS â†’ ìë§‰ â†’ ë Œë”ë§ê¹Œì§€ 1ê°œ ì˜ìƒ ì™„ì„±

 ì‚¬ìš©ë²•: python perfect_one_shot.py
 í•„ìš”: pip install edge-tts google-generativeai requests beautifulsoup4
 í™˜ê²½ë³€ìˆ˜: GOOGLE_API_KEY (Geminiìš©)
============================================================================
"""
import argparse
import asyncio
import functools
import io
import json
import logging
import os
import random
import re
import subprocess
import sys
import time
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Optional, Sequence, Type, TypeVar

F = TypeVar("F", bound=Callable[..., Any])

logger = logging.getLogger("youshorts")

# .env íŒŒì¼ ë¡œë“œ (GOOGLE_API_KEY ë“±)
try:
    from dotenv import load_dotenv
    load_dotenv(Path(__file__).resolve().parent / ".env")
except ImportError:
    pass  # python-dotenv ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ ì§ì ‘ ì„¤ì • í•„ìš”

# Windows cp949 ì½˜ì†” ìœ ë‹ˆì½”ë“œ ì¶œë ¥ ëŒ€ì‘
if sys.stdout and sys.stdout.encoding and sys.stdout.encoding.lower() != "utf-8":
    sys.stdout = io.TextIOWrapper(
        sys.stdout.buffer, encoding="utf-8", errors="replace"
    )
    sys.stderr = io.TextIOWrapper(
        sys.stderr.buffer, encoding="utf-8", errors="replace"
    )


# ============================================================================
# ì„¤ì •
# ============================================================================
class Config:
    """í”„ë¡œì íŠ¸ ì „ì²´ ì„¤ì • - constants.py ì—­í• """

    # â”€â”€ ê²½ë¡œ â”€â”€
    BASE_DIR = Path(__file__).resolve().parent
    OUTPUT_DIR = BASE_DIR / "output"
    BGM_DIR = BASE_DIR / "data" / "bgm"
    BG_DIR = BASE_DIR / "data" / "backgrounds"
    HISTORY_FILE = BASE_DIR / "data" / "history.json"

    # â”€â”€ ì˜ìƒ ìŠ¤í™ â”€â”€
    WIDTH = 1080
    HEIGHT = 1920
    FPS = 30
    MAX_DURATION = 59  # ìˆì¸  ì œí•œ 60ì´ˆ ë¯¸ë§Œ

    # â”€â”€ TTS â”€â”€
    TTS_VOICE = "ko-KR-InJoonNeural"  # ë‚¨ì„± ìŒì„± (SunHi=ì—¬ì„±â†’InJoon=ë‚¨ì„±)
    TTS_RATE = "+10%"
    TTS_PITCH = "+0Hz"

    # â”€â”€ ìë§‰ â”€â”€
    SUBTITLE_FONT = "Malgun Gothic"
    SUBTITLE_SIZE = 62
    SUBTITLE_COLOR_NORMAL = "&H00FFFFFF"
    SUBTITLE_COLOR_HIGHLIGHT = "&H0000FFFF"
    SUBTITLE_OUTLINE = 4
    SUBTITLE_SHADOW = 2
    SUBTITLE_MARGIN_V = 350

    # â”€â”€ í’ˆì§ˆ â”€â”€
    MIN_QUALITY_SCORE = 80
    MAX_RETRY = 5

    # â”€â”€ AI ìŠ¬ë¡­ ê¸ˆì§€ì–´ (ëŒ€í­ í™•ì¥) â”€â”€
    AI_SLOP_WORDS = [
        # ê²©ì‹ì²´/ë‰´ìŠ¤ì²´
        "í¥ë¯¸ë¡­", "ë†€ë¼ìš´", "ì¶©ê²©ì ", "ì‹¬ì¸µ", "íƒêµ¬", "ì—¬ì •",
        "ì•Œì•„ë³´ê² ", "ì‚´í´ë³´ê² ", "í•¨ê»˜ ì•Œì•„", "ê·¸ë ‡ë‹¤ë©´",
        "~ì¸ ì…ˆì´ë‹¤", "~ë¼ í•  ìˆ˜ ìˆ", "ê²°ë¡ ì ìœ¼ë¡œ",
        "ë§ˆë¬´ë¦¬í•˜ë©°", "ì •ë¦¬í•˜ìë©´", "ìš”ì•½í•˜ìë©´",
        "ì£¼ëª©í•  ë§Œí•œ", "ëˆˆì—¬ê²¨ë³¼", "í•œí¸ìœ¼ë¡œëŠ”", "ë‹¤ë¥¸ í•œí¸ìœ¼ë¡œëŠ”",
        "ì£¼ëª©í•´ì•¼", "ê¹Šì´ ìˆëŠ”", "ì˜ë¯¸ ìˆëŠ”", "ë‹¤ì–‘í•œ ì¸¡ë©´",
        "ì‹œì‚¬í•˜ëŠ” ë°”", "ê·€ì¶”ê°€ ì£¼ëª©", "ì „ë¬¸ê°€ë“¤ì€", "ê´€ê³„ìì— ë”°ë¥´ë©´",
        "ì´ëª©ì´ ì§‘ì¤‘", "í™”ì œë¥¼ ëª¨ìœ¼", "ë…¼ë€ì´ ë˜ê³ ",
        # AI íŠ¹ìœ ì˜ ê³¼ì‰ ì„œìˆ 
        "ë§¤ë ¥ì ì¸", "ì¸ìƒì ì¸", "ë…ë³´ì ì¸", "í˜ì‹ ì ì¸",
        "íšê¸°ì ì¸", "ì••ë„ì ì¸", "ê²½ì´ë¡œìš´", "ë†€ëê²Œë„",
        "í¥ë¯¸ì§„ì§„", "ê°íƒ„ì„ ìì•„", "ëˆˆê¸¸ì„ ë„",
        # AI íŠ¹ìœ ì˜ ì—°ê²°ì–´
        "ê·¸ë¿ë§Œ ì•„ë‹ˆë¼", "ì´ì— ë”í•´", "ë‚˜ì•„ê°€", "ë”ë¶ˆì–´",
        "í•œ ê±¸ìŒ ë”", "ì´ì–´ì„œ", "ë§ë¶™ì´ìë©´",
        # AI íŠ¹ìœ ì˜ ë§ˆë¬´ë¦¬
        "ë˜ìƒˆê²¨ ë³´", "ëŒì•„ë³´ë©´", "ê³°ê³°ì´ ìƒê°",
        "ê¹Šì€ ìš¸ë¦¼", "í° êµí›ˆ", "ì‹œì‚¬í•˜ëŠ” ì ",
        # ê²©ì‹ ë†’ì„ (ìˆì¸  ë¶€ì í•©)
        "í•˜ê² ìŠµë‹ˆë‹¤", "ë“œë¦¬ê² ìŠµë‹ˆë‹¤", "ë§ì”€ë“œë¦¬",
        "ë˜ê² ìŠµë‹ˆë‹¤", "ë˜ì‹œê² ìŠµë‹ˆê¹Œ",
    ]

    # â”€â”€ ìƒì‚° í•œë„ â”€â”€
    MAX_PER_DAY = 10

    # â”€â”€ ì´ëª¨ì§€ íŒ¨í„´ â”€â”€
    EMOJI_PATTERN = re.compile(
        "[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿"
        "ğŸš€-ğŸ›¿ğŸ‡ -ğŸ‡¿"
        "âœ‚-â°ï¸€-ï¸"
        "ğŸ¤€-ğŸ§¿ğŸ¨€-ğŸ©¯]"
    )

    # â”€â”€ YouTube â”€â”€
    YOUTUBE_PRIVACY = "public"

    # â”€â”€ FFmpeg â”€â”€
    FFMPEG_EXE = "ffmpeg"
    FFPROBE_EXE = "ffprobe"

    # â”€â”€ ì£¼ì œ í•„í„° â”€â”€
    TOPIC_BLACKLIST = [
        # ì •ì¹˜ (ì€ì–´/ë¹„ì†ì–´ í¬í•¨)
        "êµ­íšŒ", "ëŒ€í†µë ¹", "íƒ„í•µ", "ì—¬ë‹¹", "ì•¼ë‹¹", "ë¯¼ì£¼ë‹¹", "êµ­ë¯¼ì˜í˜",
        "ì´ì„ ", "ì„ ê±°", "í›„ë³´", "ì •ë‹¹", "ì˜ì›", "ì²­ì™€ëŒ€", "ì •ë¶€",
        "ì™¸êµ", "ë¶í•œ", "í•œë¯¸", "ì •ìƒíšŒë‹´", "êµ­ë°©", "ì•ˆë³´",
        "ì¹œë…¸", "ì¹œë¬¸", "ì¹œìœ¤", "ì¢ŒíŒŒ", "ìš°íŒŒ", "ì§„ë³´", "ë³´ìˆ˜",
        "ì´ì¬ëª…", "ìœ¤ì„ì—´", "í•œë™í›ˆ", "ì´ë‚™ì—°", "ì†í•™ê·œ",
        "ë˜¥íŒŒë¦¬", "ì†ê°€í˜", "ë¬¸ë¹ ", "ìœ¤ë¹ ", "êµ­ì§", "ë¯¼ì§œ",
        "í•µí˜‘ìƒ", "ì†ë³´", "ê¸´ê¸‰",
        # ê²½ì œ
        "ê¸ˆë¦¬", "í™˜ìœ¨", "ì¦ì‹œ", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì£¼ê°€", "GDP",
        "ë¬¼ê°€", "ì¸í”Œë ˆì´ì…˜", "ê¸°ì¤€ê¸ˆë¦¬", "í•œì€", "êµ­ì±„",
        # ì‚¬ê±´ì‚¬ê³ 
        "ì‚¬ë§", "ì‚¬ê³ ", "í™”ì¬", "ì§€ì§„", "íƒœí’", "í­ë°œ", "ì¶”ëª¨",
        "ìœ ì¡±", "í¬ìƒ", "ì°¸ì‚¬", "ì‹¤ì¢…", "ë¶•ê´´",
        # ë²•ë¥ 
        "ì¬íŒ", "íŒê²°", "êµ¬ì†", "ê¸°ì†Œ", "ê²€ì°°", "ê²½ì°°", "ìˆ˜ì‚¬",
        "í”¼ì˜ì", "í˜ì˜", "ì²´í¬", "ì†¡ì¹˜",
        # í–‰ì •
        "êµ­ë¬´íšŒì˜", "ì˜ˆì‚°", "ë²•ì•ˆ", "ì¡°ë¡€", "ê°ì‚¬ì›", "ê·œì œ",
        # ì»¤ë®¤ë‹ˆí‹° ì¡ê¸€ (ê³µì§€/ê´‘ê³ /ëª¨ì§‘/ì§ˆë¬¸)
        "ê³µì§€", "í†µí•©", "ì²´í—˜ë‹¨", "ëª¨ì§‘", "ì´ë²¤íŠ¸", "ê´‘ê³ ", "ì œíœ´",
        "ìŠ¤í¬", "ì§ˆë¬¸ë“œë¦½ë‹ˆë‹¤", "ì§ˆë¬¸ìˆìŠµë‹ˆë‹¤", "ë¬¸ì˜", "ì•ˆë‚´",
        "êµ¬ì¸", "êµ¬ì§", "íŒë‹ˆë‹¤", "ì‚½ë‹ˆë‹¤", "í•œì¤„í‰", "ì„¤ë¬¸",
        # ìˆì¸  ë¶€ì í•©
        "ë°¥ìƒ", "ëª…ì ˆ", "ì‹œì–´ë¨¸ë‹ˆ", "ë©°ëŠë¦¬",
        "íƒì‹œ", "ì‹¬ì¿µ", "ë¡œë§¨ìŠ¤",
        # ì‹œì¦Œ/ëª…ì ˆ ì´ìŠˆ (ì§€ë‚œ ì´ìŠˆ ë°°ì œ)
        "ì„¤ë‚ ", "ìƒˆí•´", "ì¶”ì„", "í•œê°€ìœ„", "í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ì„±íƒ„ì ˆ",
        "ë°œë Œíƒ€ì¸", "í™”ì´íŠ¸ë°ì´", "ì–´ë²„ì´ë‚ ", "ìŠ¤ìŠ¹ì˜ë‚ ",
        "ì¡¸ì—…ì‹", "ì…í•™ì‹", "ìˆ˜ëŠ¥", "ìˆ˜ëŠ¥ë‚ ",
        # ê´‘ê³ /ìŠ¤íŒ¸
        "í…”ë ˆê·¸ë¨", "ë‹¨í†¡ë°©", "ì¹´í†¡ë°©", "ì˜¤í”ˆì±„íŒ…",
        "ë¹„íŠ¸ì½”ì¸", "ê°€ìƒí™”í", "ì½”ì¸", "NFT",
        "íˆ¬ì", "ìˆ˜ìµë¥ ", "ì›ê¸ˆë³´ì¥",
        "ë¬´ë£Œë‚˜ëˆ”", "ì„ ì°©ìˆœ", "í• ì¸ì½”ë“œ",
        # ì˜ë£Œ/ê±´ê°• í—ˆìœ„ì •ë³´ ìœ„í—˜
        "ì•” ì¹˜ë£Œ", "íŠ¹íš¨ì•½", "ë¯¼ê°„ìš”ë²•", "ìê°€ì§„ë‹¨",
        "ë³‘ì›ì—ì„œ ì•ˆ ì•Œë ¤ì£¼ëŠ”", "ì˜ì‚¬ê°€ ìˆ¨ê¸°ëŠ”",
        # ì„±ì¸/ë¶€ì ì ˆ
        "í›„ë°©ì£¼ì˜", "19ê¸ˆ", "ì€ê¼´",
    ]

    # ë°”ì´ëŸ´ ì‹ í˜¸ í‚¤ì›Œë“œ (ë¸Œëœë“œëª… ì œê±°, ë°˜ì‘/ê°ì •/í–‰ë™ í‚¤ì›Œë“œ ìœ„ì£¼)
    TOPIC_BOOST_KEYWORDS = [
        "ë ˆì „ë“œ", "ì‹¤í™”", "ëŒ€ë°•", "ë¯¸ì³¤", "ì†Œë¦„", "ë…¼ë€",
        "ë°˜ì „", "í›„ê¸°", "ë¨¹ë°©", "ê²Œì„", "ë¦¬ë·°",
        "ì•„ì´ëŒ", "ë“œë¼ë§ˆ", "ì˜í™”", "ì›¹íˆ°",
        "ë°ˆ", "ì±Œë¦°ì§€", "í•«", "í„°ì§", "ë‚œë¦¬",
        "ë¹„êµ", "ë­í‚¹", "ìˆœìœ„", "VS", "TOP",
        "ê¿€íŒ", "í•´ë´„", "ì¨ë´„", "ì‚¬ë´„", "ê°€ë´„",
        # 2030 íƒ€ê²Ÿ ë¶€ìŠ¤íŠ¸
        "ì›”ê¸‰", "í‡´ì‚¬", "ì•¼ê·¼", "ìì·¨", "ì›”ì„¸", "ì „ì„¸",
        "ì‚¬íšŒì´ˆë…„ìƒ", "ì§ì¥ìƒì‚¬", "ê¼°ëŒ€", "MZ", "ì›Œë¼ë°¸",
        "ì—°ë´‰", "ì´ì§", "ì•Œë°”", "ë©´ì ‘", "ì·¨ì¤€",
        "ì¸", "ì†Œê°œíŒ…", "ê²°í˜¼", "ì¶•ì˜ê¸ˆ", "ì²­ì²©ì¥",
        "ì—°ì• ", "ì¬í…Œí¬", "ê³ ë°±", "ì ê¸ˆ", "ì²­ì•½",
    ]

    # â”€â”€ ìˆì¸  í­ë°œë ¥ ì¹´í…Œê³ ë¦¬ (ì¡°íšŒìˆ˜ 100ë§Œ+ ì‹¤ì  ê¸°ë°˜) â”€â”€
    VIRAL_CATEGORY_KEYWORDS = {
        # Tier S: 100ë§Œë·° í™•ë¥  ë†’ì€ ì¹´í…Œê³ ë¦¬
        "ê³µí¬_ë¯¸ìŠ¤í„°ë¦¬": {
            "keywords": ["ê³µí¬", "ê·€ì‹ ", "ì‹¬ë ¹", "ë¯¸ìŠ¤í„°ë¦¬", "ì†Œë¦„", "ë¬´ì„œìš´", "ê´´ë‹´",
                          "í˜¸ëŸ¬", "íê±´ë¬¼", "ë„ì‹œì „ì„¤", "ë‚©ê³¨ë‹¹", "ì €ì£¼"],
            "boost": 50000,
        },
        "ë†€ë¼ìš´_ì‚¬ì‹¤": {
            "keywords": ["ì¶©ê²©", "ì•Œê³ ë³´ë‹ˆ", "ì§„ì‹¤", "ëª°ëë˜", "ë¹„ë°€", "ë°˜ì „",
                          "ì‹¤í™”", "ë ˆì „ë“œ", "ì—­ëŒ€ê¸‰", "ë¯¸ì³¤", "ê²½ì•…"],
            "boost": 45000,
        },
        "ë°ˆ_ìœ ë¨¸": {
            "keywords": ["ë°ˆ", "ì§¤", "ã…‹ã…‹", "ì›ƒê¸´", "ê°œì›ƒ", "ì¡´ì›ƒ", "í‚¹ë°›",
                          "ë¹¡ì¹¨", "ì–´ì´ì—†", "í™©ë‹¹", "í•´í”„ë‹", "ì›ƒì°¸"],
            "boost": 40000,
        },
        "ë¹„êµ_ë­í‚¹": {
            "keywords": ["ë¹„êµ", "VS", "ë­í‚¹", "ìˆœìœ„", "TOP", "1ìœ„",
                          "ìµœê³ ", "ìµœì•…", "ì°¨ì´", "ì–´ë–¤ê²Œ", "ë­ê°€"],
            "boost": 40000,
        },
        "ê¿€íŒ_ë¼ì´í”„í•µ": {
            "keywords": ["ê¿€íŒ", "ë°©ë²•", "ë…¸í•˜ìš°", "í•µê¿€", "ê°œê¿€", "íŒ",
                          "ê°€ì„±ë¹„", "ì•Œëœ°", "ì €ë ´", "ì•„ë¼ëŠ”", "ì ˆì•½"],
            "boost": 35000,
        },
        "ë¬¸í™”ì¶©ê²©_ë°˜ì‘": {
            "keywords": ["ì™¸êµ­ì¸", "ë¬¸í™”ì¶©ê²©", "ë°˜ì‘", "í•œêµ­", "í•´ì™¸", "ë¦¬ì•¡ì…˜",
                          "ì¶©ê²©ë°›", "ë†€ë€", "ì°¨ì´ì ", "ë¹„êµë¬¸í™”"],
            "boost": 45000,
        },
        "ê²Œì„_ì• ë‹ˆ": {
            "keywords": ["ê²Œì„", "ë¡¤", "ë°œë¡œë€íŠ¸", "ë§ˆí¬", "ì›ì‹ ", "ì• ë‹ˆ",
                          "ì›í”¼ìŠ¤", "ê·€ë©¸", "ë‚˜ë£¨í† ", "ì£¼ìˆ íšŒì „", "ì§„ê²©"],
            "boost": 35000,
        },
    }

    # â”€â”€ ìˆì¸  ë¶€ì í•© ê°ì  ì¹´í…Œê³ ë¦¬ (ì¡°íšŒìˆ˜ ë‚®ì€ ìœ í˜•) â”€â”€
    BORING_PENALTY_PATTERNS = [
        # ì¼ìƒ ì¡ë‹´ (ìˆì¸ ì—ì„œ ì•ˆ í„°ì§)
        (r"ë‚¨ì¹œ|ì—¬ì¹œ|ë‚¨ìì¹œêµ¬|ì—¬ìì¹œêµ¬|ì„¤ê±°ì§€|ì‹œëŒ|ì‹œì–´ë¨¸ë‹ˆ|ê²°í˜¼ì‹", -30000, "ì—°ì• /ê²°í˜¼ ì¼ìƒ"),
        (r"íšŒì‚¬|ì§ì¥|í‡´ê·¼|ì¶œê·¼|ì•¼ê·¼|ìƒì‚¬|ì„ ë°°|ì‹ ì…", -20000, "ì§ì¥ ì¼ìƒ"),
        (r"ë‹¤ì´ì–´íŠ¸|ì‹ë‹¨|ìš´ë™|í—¬ìŠ¤", -15000, "ë‹¤ì´ì–´íŠ¸ ì¼ìƒ"),
        (r"ì¹´í˜|ë§›ì§‘|ë””ì €íŠ¸|ë¹µì§‘", -15000, "ì¹´í˜/ë§›ì§‘ ì¼ìƒ"),
        # ì—°ì˜ˆ ë‹¨ìˆœ ê°€ì‹­ (ê¹Šì´ ì—†ëŠ” ê²ƒ)
        (r"ì—´ì• |ê²°ë³„|ì†Œì†ì‚¬|ì»´ë°±|ì•¨ë²”|íŒ¬ì‹¸", -10000, "ì—°ì˜ˆ ë‹¨ìˆœë‰´ìŠ¤"),
    ]

    # â”€â”€ ì£¼ì œë³„ ë°°ê²½ ëª¨ë“œ â”€â”€
    TOPIC_BG_MAP: dict[str, list[str]] = {
        "gameplay": ["ê²Œì„", "ë¡¤", "ì„œí¼", "ì±Œë¦°ì§€", "ë°ˆ", "ì§¤", "ë§ˆí¬", "ë°œë¡œë€íŠ¸"],
        "gradient": [
            "ë§›ì§‘", "í¸ì˜ì ", "ì¹´í˜", "ìŒì‹", "ìš”ë¦¬", "ë ˆì‹œí”¼",
            "ë‹¤ì´ì†Œ", "ì˜¬ë¦¬ë¸Œì˜", "ë·°í‹°", "íŒ¨ì…˜", "ê¿€íŒ", "ê°€ì„±ë¹„",
            "ìì·¨", "ê¿€ì¡°í•©", "ì‹ ìƒ", "í›„ê¸°", "ë¨¹ë°©",
        ],
    }

    # ì£¼ì œ í‚¤ì›Œë“œ â†’ ê·¸ë¼ë””ì–¸íŠ¸ ìƒ‰ìƒ (ìƒë‹¨â†’í•˜ë‹¨, ì–´ë‘¡ì§€ë§Œ í™•ì‹¤íˆ ìƒ‰ê° ë³´ì´ëŠ” í†¤)
    GRADIENT_COLORS: dict[str, tuple[str, str]] = {
        "food":    ("#1C1C1C", "#3D2B1F"),  # ì°¨ì½œ â†’ ë‹¤í¬ë¸Œë¼ìš´ (ë”°ëœ»í•œ í†¤)
        "beauty":  ("#1C1C1C", "#2D2D3F"),  # ì°¨ì½œ â†’ ë‹¤í¬ìŠ¬ë ˆì´íŠ¸ (ì°¨ê°€ìš´ í†¤)
        "info":    ("#1C1C1C", "#1A2A3A"),  # ì°¨ì½œ â†’ ë‹¤í¬ë„¤ì´ë¹„
        "default": ("#1C1C1C", "#2A2A2A"),  # ì°¨ì½œ â†’ ë‹¤í¬ê·¸ë ˆì´ (ë¬´ì±„ìƒ‰)
    }
    GRADIENT_TOPIC_MAP: dict[str, list[str]] = {
        "food":   ["ë§›ì§‘", "í¸ì˜ì ", "ìŒì‹", "ìš”ë¦¬", "ë ˆì‹œí”¼", "ë¨¹ë°©", "ê¿€ì¡°í•©", "ì¹´í˜", "ë§¥ë„ë‚ ë“œ", "ìŠ¤íƒ€ë²…ìŠ¤"],
        "beauty": ["ì˜¬ë¦¬ë¸Œì˜", "ë·°í‹°", "íŒ¨ì…˜", "ë‹¤ì´ì†Œ", "í™”ì¥", "ìŠ¤í‚¨"],
        "info":   ["ê¿€íŒ", "ê°€ì„±ë¹„", "í›„ê¸°", "ìì·¨", "ì‹ ìƒ", "ì •ë³´"],
    }


# ============================================================================
# retry ë°ì½”ë ˆì´í„° (ì§€ìˆ˜ ë°±ì˜¤í”„ + ì§€í„°)
# ============================================================================
DEFAULT_RETRYABLE: tuple[Type[BaseException], ...] = (
    ConnectionError, TimeoutError, OSError,
)


def retry(
    max_retries: int = 3,
    backoff_factor: float = 2.0,
    jitter: float = 0.5,
    retryable_exceptions: Sequence[Type[BaseException]] = DEFAULT_RETRYABLE,
) -> Callable[[F], F]:
    """API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„ (ì§€ìˆ˜ ë°±ì˜¤í”„)."""
    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            last_exc: BaseException | None = None
            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except tuple(retryable_exceptions) as e:
                    last_exc = e
                    if attempt < max_retries:
                        wait = backoff_factor ** attempt + random.uniform(0, jitter)
                        print(f"  [RETRY] {func.__name__} ({attempt+1}/{max_retries+1}): {e}")
                        time.sleep(wait)
            raise last_exc  # type: ignore
        return wrapper  # type: ignore
    return decorator


# ============================================================================
# ì¼ì¼ ìƒì‚° í•œë„ ì²´í¬
# ============================================================================
def check_daily_limit() -> bool:
    """ì˜¤ëŠ˜ ìƒì‚° ê°œìˆ˜ê°€ MAX_PER_DAY ì´í•˜ì¸ì§€ í™•ì¸."""
    if not Config.HISTORY_FILE.exists():
        return True
    try:
        data = json.loads(Config.HISTORY_FILE.read_text(encoding="utf-8"))
        today = datetime.now().strftime("%Y-%m-%d")
        today_count = sum(
            1 for item in data
            if item.get("created_at", "").startswith(today)
        )
        if today_count >= Config.MAX_PER_DAY:
            print(f"  [WARN] ì¼ì¼ í•œë„ ë„ë‹¬: {today_count}/{Config.MAX_PER_DAY}ê°œ")
            return False
        print(f"  ì˜¤ëŠ˜ ìƒì‚°: {today_count}/{Config.MAX_PER_DAY}ê°œ")
        return True
    except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:
        logger.warning(f"ì¼ì¼ í•œë„ í™•ì¸ ì‹¤íŒ¨: {e}")
        return True


# ============================================================================
# FFmpeg / FFprobe ê²½ë¡œ íƒìƒ‰
# ============================================================================
def _find_ffmpeg_exe() -> str:
    """imageio_ffmpeg ìš°ì„ , ì—†ìœ¼ë©´ PATHì˜ ffmpeg."""
    try:
        import imageio_ffmpeg
        return imageio_ffmpeg.get_ffmpeg_exe()
    except ImportError:
        return "ffmpeg"


def _find_ffprobe_exe() -> str:
    """imageio_ffmpeg ê¸°ë°˜ ffprobe ê²½ë¡œ."""
    try:
        import imageio_ffmpeg
        ffmpeg_path = imageio_ffmpeg.get_ffmpeg_exe()
        ffprobe_path = ffmpeg_path.replace("ffmpeg", "ffprobe")
        if os.path.exists(ffprobe_path):
            return ffprobe_path
    except ImportError:
        pass
    return "ffprobe"


# ============================================================================
# STEP 1: íŠ¸ë Œë“œ ìˆ˜ì§‘ - 3ê°œ ì†ŒìŠ¤ + APIFY
# ============================================================================
class TrendCollector:
    """
    íŠ¸ë Œë“œ ìˆ˜ì§‘ ì „ëµ:
    Google Trends RSS + ë„¤ì´ë²„ ì‹œê·¸ë„ + ì»¤ë®¤ë‹ˆí‹° í•«ê¸€ + APIFY
    â†’ ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„° â†’ ë¶€ìŠ¤íŠ¸ ì ìš© â†’ ì¤‘ë³µ ì œê±° â†’ TOP 1 ì„ ì •
    """

    def __init__(self):
        self.trends = []

    def fetch_google_trends_rss(self) -> list[dict]:
        """RSS í”¼ë“œë¼ API í‚¤ ë¶ˆí•„ìš”, IP ì°¨ë‹¨ 0%"""
        import requests

        url = "https://trends.google.co.kr/trending/rss?geo=KR"
        results = []

        try:
            resp = requests.get(url, timeout=5, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 Chrome/120.0.0.0"
            })
            resp.raise_for_status()

            root = ET.fromstring(resp.text)
            ns = {"ht": "https://trends.google.co.kr/trending/rss"}

            for idx, item in enumerate(root.findall(".//item")):
                title = item.find("title")
                if title is not None and title.text:
                    traffic = item.find("ht:approx_traffic", ns)
                    traffic_num = 0
                    if traffic is not None and traffic.text:
                        traffic_num = int(
                            traffic.text.replace(",", "").replace("+", "")
                        )
                    # traffic ê°’ ì—†ìœ¼ë©´ ìˆœìœ„ ê¸°ë°˜ ê¸°ë³¸ì ìˆ˜ (ì‹¤ê²€ì´ë‹ˆ ìµœì†Œ 10,000)
                    if traffic_num == 0:
                        traffic_num = max(50000 - idx * 3000, 10000)
                    results.append({
                        "keyword": title.text.strip(),
                        "source": "google_trends",
                        "score": traffic_num,
                    })

            print(f"  [OK] Google Trends: {len(results)}ê°œ ìˆ˜ì§‘")

        except Exception as e:
            print(f"  [WARN] Google Trends ì‹¤íŒ¨: {e}")

        return results

    def fetch_naver_realtime(self) -> list[dict]:
        """ë„¤ì´ë²„ ì‹¤ì‹œê°„ ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´ (ì—°ê´€ê²€ìƒ‰ì–´ API í™œìš©)"""
        import requests
        from bs4 import BeautifulSoup

        results = []

        # 1) ë„¤ì´ë²„ ëª¨ë°”ì¼ ë©”ì¸ ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´
        try:
            url = "https://m.search.naver.com/search.naver?query=%EC%8B%A4%EC%8B%9C%EA%B0%84+%EA%B8%89%EC%83%81%EC%8A%B9"
            resp = requests.get(url, timeout=5, headers={
                "User-Agent": "Mozilla/5.0 (Linux; Android 13; SM-S908B) "
                              "AppleWebKit/537.36 Chrome/120.0.0.0 Mobile Safari/537.36",
            })
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, "html.parser")
                # ê¸‰ìƒìŠ¹ ê²€ìƒ‰ì–´ í•­ëª©
                items = soup.select(".lst_relate .item") or soup.select("a.keyword")
                for i, item in enumerate(items[:15]):
                    text = item.get_text(strip=True)
                    if text and len(text) > 1:
                        results.append({
                            "keyword": text,
                            "source": "naver_realtime",
                            "score": (15 - i) * 5000,
                        })
        except (ConnectionError, TimeoutError, Exception) as e:
            logger.warning(f"ìš”ì²­ ì‹¤íŒ¨: {e}")

        # 2) ë„¤ì´ë²„ ì‡¼í•‘ ì¸ê¸° ê²€ìƒ‰ì–´ (ì†Œë¹„ íŠ¸ë Œë“œ = ì‡¼ì¸  ì£¼ì œ ì í•©)
        try:
            url2 = "https://search.shopping.naver.com/search/all?query=%EC%9D%B8%EA%B8%B0&sort=rel"
            resp2 = requests.get(url2, timeout=5, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 Chrome/120.0.0.0",
            })
            if resp2.status_code == 200:
                soup2 = BeautifulSoup(resp2.text, "html.parser")
                items2 = soup2.select(".relateKeyword_relation_item__key") or []
                for i, item in enumerate(items2[:10]):
                    text = item.get_text(strip=True)
                    if text and len(text) > 1:
                        results.append({
                            "keyword": text,
                            "source": "naver_shopping",
                            "score": (10 - i) * 4000,
                        })
        except (ConnectionError, TimeoutError, Exception) as e:
            logger.warning(f"ìš”ì²­ ì‹¤íŒ¨: {e}")

        if results:
            print(f"  [OK] ë„¤ì´ë²„ ì‹¤ì‹œê°„: {len(results)}ê°œ ìˆ˜ì§‘")
        else:
            print(f"  [WARN] ë„¤ì´ë²„ ì‹¤ì‹œê°„ ìˆ˜ì§‘ ì‹¤íŒ¨")

        return results

    def fetch_community_hot(self) -> list[dict]:
        """ì»¤ë®¤ë‹ˆí‹° ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ â€” ë´‡ì°¨ë‹¨ ì—†ëŠ” ì‚¬ì´íŠ¸ ìœ„ì£¼"""
        import requests
        from bs4 import BeautifulSoup

        results = []
        communities = [
            {
                "name": "ë„¤ì´íŠ¸íŒ",
                "url": "https://pann.nate.com/talk/ranking",
                "title_sel": ".tlt",
                "base_url": "https://pann.nate.com",
            },
            {
                "name": "í´ë¦¬ì•™",
                "url": "https://www.clien.net/service/board/park",
                "title_sel": ".list_subject .subject_fixed",
                "base_url": "https://www.clien.net",
            },
            {
                "name": "ë£¨ë¦¬ì›¹",
                "url": "https://bbs.ruliweb.com/community/board/300143/best",
                "title_sel": ".subject .deco",
                "base_url": "https://bbs.ruliweb.com",
            },
            {
                "name": "ë”ì¿ ",
                "url": "https://theqoo.net/hot",
                "title_sel": ".tit_topic a, .title a",
                "base_url": "https://theqoo.net",
            },
        ]

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 Chrome/120.0.0.0",
            "Accept-Language": "ko-KR,ko;q=0.9",
        }

        for comm in communities:
            try:
                resp = requests.get(
                    comm["url"], timeout=8, headers=headers,
                )
                if resp.status_code != 200:
                    continue

                soup = BeautifulSoup(resp.text, "html.parser")
                titles = soup.select(comm["title_sel"])

                count = 0
                for i, t in enumerate(titles[:10]):
                    text = t.get_text(strip=True)
                    text = re.sub(r'\d{2,}$', '', text).strip()

                    if not text or len(text) < 5 or "[ê´‘ê³ ]" in text:
                        continue

                    href = t.get("href", "")
                    if href and not href.startswith("http"):
                        href = comm["base_url"] + href

                    # â”€â”€ ì°¸ì—¬ë„ ì¶”ì¶œ (ì¡°íšŒìˆ˜/ëŒ“ê¸€ìˆ˜/ì¶”ì²œìˆ˜) â”€â”€
                    engagement_score = 0
                    parent = t.find_parent("tr") or t.find_parent("div") or t.find_parent("li")
                    if parent:
                        parent_text = parent.get_text()
                        # ì¡°íšŒìˆ˜ (ì¼ë°˜ì ì¸ íŒ¨í„´)
                        view_m = re.search(r'(\d{1,3}(?:,\d{3})*)\s*(?:ì¡°íšŒ|ì½ìŒ|hit)', parent_text)
                        if view_m:
                            engagement_score += int(view_m.group(1).replace(",", "")) * 0.01
                        # ëŒ“ê¸€ìˆ˜ (ìˆ«ì + [] íŒ¨í„´)
                        cmt_m = re.search(r'\[(\d+)\]', parent_text)
                        if cmt_m:
                            engagement_score += int(cmt_m.group(1)) * 1.5
                        # ì¶”ì²œìˆ˜
                        rec_m = re.search(r'(\d+)\s*(?:ì¶”ì²œ|ê³µê°|ì¢‹ì•„ìš”)', parent_text)
                        if rec_m:
                            engagement_score += int(rec_m.group(1)) * 2.0

                    # ìœ„ì¹˜ ê¸°ë°˜ + ì°¸ì—¬ë„ ë³µí•© ì ìˆ˜
                    base_score = (10 - i) * 3000
                    results.append({
                        "keyword": text,
                        "source": f"community_{comm['name']}",
                        "score": base_score + engagement_score,
                        "url": href,
                        "body": "",
                    })
                    count += 1

                print(f"  [OK] {comm['name']}: {count}ê°œ")

            except Exception as e:
                print(f"  [WARN] {comm['name']} ì‹¤íŒ¨: {e}")

        return results

    def fetch_post_body(self, url: str) -> str:
        """ê²Œì‹œê¸€ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§"""
        import requests
        from bs4 import BeautifulSoup

        if not url:
            return ""

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 Chrome/120.0.0.0",
            "Accept-Language": "ko-KR,ko;q=0.9",
        }

        try:
            resp = requests.get(url, timeout=8, headers=headers)
            if resp.status_code != 200:
                return ""

            soup = BeautifulSoup(resp.text, "html.parser")
            body_selectors = [
                "#contentArea", ".posting_area", "#content",
                ".rd_body", ".xe_content", "article",
                ".memo_content", ".post_content",
            ]

            for sel in body_selectors:
                body_el = soup.select_one(sel)
                if body_el:
                    text = body_el.get_text(separator="\n", strip=True)
                    if len(text) > 30:
                        return text[:2000]

        except Exception as e:
            logger.warning(f"ë³¸ë¬¸ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

        return ""

    def collect_all(self) -> list[dict]:
        """ê¸°ë³¸ 3ê°œ ì†ŒìŠ¤ + APIFY í•©ì‚°, ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„° + ë¶€ìŠ¤íŠ¸"""
        print("\n" + "=" * 60)
        print("STEP 1: íŠ¸ë Œë“œ ìˆ˜ì§‘")
        print("=" * 60)

        all_trends = []
        all_trends.extend(self.fetch_google_trends_rss())
        all_trends.extend(self.fetch_naver_realtime())
        all_trends.extend(self.fetch_community_hot())

        # APIFY í¬ë¡¤ëŸ¬ (í† í° ìˆìœ¼ë©´ ìë™ ì¶”ê°€)
        apify_results = ApifyCrawler.crawl()
        if apify_results:
            all_trends.extend(apify_results)
            print(f"  [OK] APIFY: {len(apify_results)}ê°œ ì¶”ê°€")

        # â”€â”€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ + ì˜ì–´/ì§§ì€ ì œëª© í•„í„°ë§ â”€â”€
        filtered = []
        blocked = 0
        for t in all_trends:
            kw = t["keyword"]
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸
            if any(bw in kw for bw in Config.TOPIC_BLACKLIST):
                blocked += 1
                continue
            # ì˜ì–´ ë¹„ìœ¨ 50% ì´ìƒì´ë©´ ì œê±° (í•œêµ­ì–´ ì‡¼ì¸ ì— ë¶€ì í•©)
            eng_chars = sum(1 for c in kw if c.isascii() and c.isalpha())
            if len(kw) > 3 and eng_chars / len(kw) > 0.5:
                blocked += 1
                continue
            # ë„ˆë¬´ ì§§ì€ í‚¤ì›Œë“œ (2ê¸€ì ì´í•˜) ì œê±°
            clean_kw = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', kw)
            if len(clean_kw) < 3:
                blocked += 1
                continue
            filtered.append(t)

        if blocked:
            print(f"  [FILTER] ë¸”ë™ë¦¬ìŠ¤íŠ¸ {blocked}ê°œ ì œê±°")

        # â”€â”€ ì»¤ë®¤ë‹ˆí‹° ì¡ê¸€ í•„í„° (ì§ˆë¬¸/ê³µì§€/ì§§ì€ ì œëª© ì œê±°) â”€â”€
        junk_patterns = ["?", "ì§ˆë¬¸", "ë­˜ê¹Œ", "ì–´ë–»ê²Œ", "í•˜ëŠ”ê±´ê°€", "ë“œë¦½ë‹ˆë‹¤",
                         "ğŸ“¢", "ì¤‘ìš”", "ë³€ê²½ ê¶Œì¥", "ê·œì¹™", "ì¹´í…Œê³ ë¦¬"]
        pre_junk = len(filtered)
        filtered = [
            t for t in filtered
            if not ("community" in t.get("source", "")
                    and (len(t["keyword"]) < 10
                         or any(jp in t["keyword"] for jp in junk_patterns)))
        ]
        junk_removed = pre_junk - len(filtered)
        if junk_removed:
            print(f"  [FILTER] ì»¤ë®¤ë‹ˆí‹° ì¡ê¸€ {junk_removed}ê°œ ì œê±°")

        # â”€â”€ ì ìˆ˜ ì¬ì¡°ì • â”€â”€
        # Google Trends: ì‹¤ì œ ê²€ìƒ‰ëŸ‰ ê¸°ë°˜ì´ë¯€ë¡œ ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ
        # ì»¤ë®¤ë‹ˆí‹°: ê²Œì‹œíŒ ê¸€ ìˆœì„œ ê¸°ë°˜, ì‡¼ì¸  ì í•©ì„± ë¶ˆí™•ì‹¤
        for t in filtered:
            src = t.get("source", "")
            if src == "google_trends":
                pass  # ê¸°ë³¸ ì ìˆ˜ ìœ ì§€
            elif "community" in src:
                t["score"] = int(t["score"] * 1.5)  # ì•½í•œ ë¶€ìŠ¤íŠ¸ë§Œ

        # â”€â”€ ë¶€ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (ì–´ëŠ ì†ŒìŠ¤ë“  ì ìš©) â”€â”€
        for t in filtered:
            kw = t["keyword"]
            boost_count = sum(1 for bk in Config.TOPIC_BOOST_KEYWORDS if bk in kw)
            if boost_count:
                t["score"] += boost_count * 20000

        # â”€â”€ ìˆì¸  í­ë°œë ¥ ì¹´í…Œê³ ë¦¬ ë¶€ìŠ¤íŠ¸ (ê°€ì¥ ì¤‘ìš”!) â”€â”€
        for t in filtered:
            kw = t["keyword"]
            best_cat = ""
            best_boost = 0
            for cat_name, cat_info in Config.VIRAL_CATEGORY_KEYWORDS.items():
                match_count = sum(1 for ck in cat_info["keywords"] if ck in kw)
                if match_count > 0 and cat_info["boost"] > best_boost:
                    best_boost = cat_info["boost"]
                    best_cat = cat_name
            if best_boost:
                t["score"] += best_boost
                t["_viral_category"] = best_cat

        # â”€â”€ ìˆì¸  ë¶€ì í•© ê°ì  (ì¼ìƒ ì¡ë‹´/ê°€ì‹­ ê±¸ëŸ¬ë‚´ê¸°) â”€â”€
        for t in filtered:
            kw = t["keyword"]
            for pat, penalty, label in Config.BORING_PENALTY_PATTERNS:
                if re.search(pat, kw):
                    t["score"] += penalty  # ìŒìˆ˜ê°’ì´ë¯€ë¡œ ê°ì 
                    break

        # â”€â”€ ì¤‘ë³µ í‚¤ì›Œë“œ í•©ì‚° (URL/body ë³´ì¡´) â”€â”€
        merged = {}
        for t in filtered:
            kw = t["keyword"]
            if kw in merged:
                merged[kw]["score"] += t["score"]
                merged[kw]["sources"].append(t["source"])
                if t.get("url") and not merged[kw].get("url"):
                    merged[kw]["url"] = t["url"]
            else:
                merged[kw] = {
                    "keyword": kw,
                    "score": t["score"],
                    "sources": [t["source"]],
                    "url": t.get("url", ""),
                }

        sorted_trends = sorted(
            merged.values(), key=lambda x: x["score"], reverse=True
        )

        print(f"\n  ì´ {len(sorted_trends)}ê°œ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì™„ë£Œ")
        for i, t in enumerate(sorted_trends[:5]):
            sources = ", ".join(t["sources"])
            print(f"  {i + 1}. [{t['score']:,}ì ] {t['keyword']} ({sources})")

        return sorted_trends


# ============================================================================
# STEP 1.5: ë‰´ìŠ¤ ë³´ê°• - íŒ©íŠ¸ ê¸°ë°˜ ëŒ€ë³¸ ì›ë£Œ
# ============================================================================
class NewsCollector:
    """íŠ¸ë Œë“œ í‚¤ì›Œë“œ â†’ Google News RSS + ë„¤ì´ë²„ ë‰´ìŠ¤ â†’ íŒ©íŠ¸ ì›ë£Œ"""

    def fetch_google_news_rss(self, keyword: str) -> list[dict]:
        import requests
        results = []
        try:
            url = (
                f"https://news.google.com/rss/search?"
                f"q={keyword}&hl=ko&gl=KR&ceid=KR:ko"
            )
            resp = requests.get(url, timeout=5)
            root = ET.fromstring(resp.text)

            for item in root.findall(".//item")[:5]:
                title = item.find("title")
                desc = item.find("description")
                if title is not None:
                    results.append({
                        "title": title.text or "",
                        "desc": (desc.text or "")[:200],
                        "source": "google_news",
                    })
            print(f"  [OK] Google News: {len(results)}ê±´")
        except Exception as e:
            print(f"  [WARN] Google News ì‹¤íŒ¨: {e}")
        return results

    def fetch_naver_news(self, keyword: str) -> list[dict]:
        import requests
        from bs4 import BeautifulSoup

        results = []
        try:
            url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
            resp = requests.get(url, timeout=5, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                              "AppleWebKit/537.36 Chrome/120.0.0.0",
            })
            soup = BeautifulSoup(resp.text, "html.parser")
            news_items = soup.select(".news_tit") or soup.select("a.news_tit")
            for item in news_items[:5]:
                results.append({
                    "title": item.get_text(strip=True),
                    "desc": "",
                    "source": "naver_news",
                })
            print(f"  [OK] ë„¤ì´ë²„ ë‰´ìŠ¤: {len(results)}ê±´")
        except Exception as e:
            print(f"  [WARN] ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹¤íŒ¨: {e}")
        return results

    def collect_news(self, keyword: str) -> list[dict]:
        print(f"\n  ë‰´ìŠ¤ ìˆ˜ì§‘: '{keyword}'")
        news = []
        news.extend(self.fetch_google_news_rss(keyword))
        news.extend(self.fetch_naver_news(keyword))
        return news


# ============================================================================
# STEP 2: ëŒ€ë³¸ ìƒì„± (Gemini 2.0 Flash â€” ë¬´ë£Œ)
# ============================================================================
class ScriptGenerator:
    """
    ëŒ€ë³¸ ìƒì„±: Gemini 2.0 Flash (ë¬´ë£Œ, ìœ ë£Œ í´ë°± ì œê±°)
    ê²€ì¦ëœ í”„ë¡¬í”„íŠ¸ + ì»¤ë®¤ë‹ˆí‹° ë³¸ë¬¸ ê¸°ë°˜ + í’ˆì§ˆ 85ì  ì´ìƒ
    """

    # â”€â”€ ì›ê¸€ ìˆì„ ë•Œ: íŒ©íŠ¸ ê¸°ë°˜ ë‚˜ë ˆì´ì…˜ â”€â”€
    PROMPT_WITH_SOURCE = """ìœ íŠœë¸Œ ìˆì¸  100ë§Œë·° ë‚˜ë ˆì´ì…˜ ëŒ€ë³¸ì„ ë§Œë“¤ì–´.

ì—­í• : 20ëŒ€ í•œêµ­ ë‚¨ìê°€ ì¹œêµ¬í•œí…Œ "ì•¼ ì´ê±° ë´ë´" í•˜ë©´ì„œ ì–˜ê¸°í•˜ëŠ” ëŠë‚Œ.

ê·œì¹™:
1. [ì›ê¸€ ë‚´ìš©]ì˜ í•µì‹¬ íŒ©íŠ¸ë¥¼ ì „ë‹¬í•´. ì—†ëŠ” ë‚´ìš© ì ˆëŒ€ ì§€ì–´ë‚´ì§€ ë§ˆ.
2. ì›ê¸€ì— ì—†ëŠ” ëŒ€í™”/ì¸ìš©/ìˆ˜ì¹˜/ë‚ ì§œë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆ.
3. ë¬¸ì¥ ê¸¸ì´ëŠ” ììœ ë¡­ê²Œ â€” ì§§ì€ ê²ƒ(5ì)ë„ ê¸´ ê²ƒ(25ì)ë„ ì„ì–´ì„œ ë¦¬ë“¬ê° ìˆê²Œ.
4. ì „ì²´ 15~22ë¬¸ì¥. 250~400ì.
5. ì²« ë¬¸ì¥ì€ ì£¼ì œë¥¼ ë°”ë¡œ êº¼ë‚´. í›… ì¡ëŠ” ì§ˆë¬¸ì´ë‚˜ í•µì‹¬ íŒ©íŠ¸ë¡œ ì‹œì‘.
   (íŒ¨í„´: ì§ˆë¬¸í˜•/ì¶©ê²©í˜•/ê³µê°í˜•/ë¹„ë°€í­ë¡œí˜•/ëŒ€ì¡°í˜• ì¤‘ íƒ1)
6. 3ì¤„ ì—°ì† ê°™ì€ ë¶„ìœ„ê¸° ê¸ˆì§€ â€” ê°ì •ì„ ê³„ì† ì „í™˜í•´ì„œ ì´íƒˆ ë°©ì§€.
7. ë§ˆì§€ë§‰ì€ ìì—°ìŠ¤ëŸ½ê²Œ ëë‚´. ì–µì§€ êµ¬ë…ìœ ë„ í•˜ì§€ ë§ˆ. ëŒ€ì‹  ì§ˆë¬¸ ë˜ì ¸ì„œ ëŒ“ê¸€ ìœ ë„.
8. ê¸ˆì§€: "ì—¬ëŸ¬ë¶„", ì‹¤ëª…, **ë³¼ë“œ**, ì´ëª¨ì§€, "êµ¬ë…", "ì¢‹ì•„ìš”"

ë§íˆ¬ ì°¸ê³  (ì´ê±¸ ê·¸ëŒ€ë¡œ ì“°ì§€ ë§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë³€í˜•í•´):
- ë†€ë„ ë•Œ: "ì´ê²Œ ì§„ì§œ?", "ì•„ ì´ê±´ ì¢€...", "ì™€ ë¯¸ì³¤ëŠ”ë°"
- ì„¤ëª…í•  ë•Œ: "ê·¼ë° ì´ê²Œ", "ì§„ì§œ ì›ƒê¸´ ê²Œ", "í¬ì¸íŠ¸ëŠ”"
- ë§ˆë¬´ë¦¬: "ì´ ì •ë„ë©´ í•´ë³¼ ë§Œí•˜ì§€", "í•œë²ˆ ì¨ë´", "ì•Œì•„ì„œ íŒë‹¨"

ì ˆëŒ€ ì“°ì§€ ë§ ê²ƒ:
{slop_words}

[ì£¼ì œ] {topic}

[ì›ê¸€ ë‚´ìš©]
{source_text}

JSONë§Œ ì¶œë ¥:
{{
  "title": "ì œëª© 15ì ì´ë‚´ (ì´ëª¨ì§€ ê¸ˆì§€)",
  "tts_script": "ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ëŒ€ë³¸",
  "tags": ["íƒœê·¸1", "íƒœê·¸2", "íƒœê·¸3", "íƒœê·¸4", "íƒœê·¸5"],
  "description": "ì„¤ëª…ë€ 2ì¤„"
}}"""

    # â”€â”€ ì›ê¸€ ì—†ì„ ë•Œ: ì£¼ì œ ê¸°ë°˜ ì •ë³´í˜• ëŒ€ë³¸ â”€â”€
    PROMPT_NO_SOURCE = """ìœ íŠœë¸Œ ì‡¼ì¸  ë‚˜ë ˆì´ì…˜ ëŒ€ë³¸ì„ ë§Œë“¤ì–´.

ì—­í• : 20ëŒ€ í•œêµ­ ë‚¨ìê°€ íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ì–˜ê¸°í•˜ëŠ” ëŠë‚Œ. ê°€ë²¼ìš´ ì •ë³´ ì „ë‹¬í˜•.

ê·œì¹™:
1. [ì£¼ì œ]ì— ëŒ€í•´ ë„ë¦¬ ì•Œë ¤ì§„ ì‚¬ì‹¤ì´ë‚˜ ëŒ€ì¤‘ì ìœ¼ë¡œ ê³µê°í•  ìˆ˜ ìˆëŠ” ë‚´ìš© ìœ„ì£¼ë¡œ ì „ë‹¬.
2. í™•ì¸ë˜ì§€ ì•Šì€ í†µê³„/ìˆ˜ì¹˜/ì—°êµ¬ê²°ê³¼ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.
3. ì§ì ‘ ê²½í—˜í•œ ê²ƒì²˜ëŸ¼ ì“°ì§€ ë§ˆ. "~ë¼ëŠ” ì‚¬ì‹¤ ì•Œì•„?" ê°™ì€ ì¼ë°˜ì  ì‚¬ì‹¤ë§Œ OK.
4. ì˜í•™/ë²•ë¥ /ê¸ˆìœµ ì •ë³´ëŠ” ì ˆëŒ€ ë‹¤ë£¨ì§€ ë§ˆ. ì˜ëª»ëœ ì •ë³´ëŠ” ìœ„í—˜í•¨.
5. ë¬¸ì¥ ê¸¸ì´ ììœ  â€” ì§§ì€ ê²ƒë„ ê¸´ ê²ƒë„ ì„ì–´ì„œ ë¦¬ë“¬ê° ìˆê²Œ.
6. ì „ì²´ 15~22ë¬¸ì¥. 250~400ì.
7. ì²« ë¬¸ì¥ì€ ì£¼ì œì˜ í•µì‹¬ì„ ë°”ë¡œ êº¼ë‚´. ì§ˆë¬¸í˜•ì´ë‚˜ ê³µê°í•  ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ë¡œ ì‹œì‘.
8. ë§ˆì§€ë§‰ì€ ìì—°ìŠ¤ëŸ½ê²Œ ëë‚´. "êµ¬ë…í•´" ê°™ì€ CTA í•˜ì§€ ë§ˆ.
9. ê¸ˆì§€: "ì—¬ëŸ¬ë¶„", ì‹¤ëª…, **ë³¼ë“œ**, ì´ëª¨ì§€, "êµ¬ë…", "ì¢‹ì•„ìš”"
10. "~ë¼ëŠ” ë§ì´ ìˆë‹¤", "ì „ë¬¸ê°€ì— ë”°ë¥´ë©´" ê°™ì€ ê·¼ê±° ì—†ëŠ” ì¸ìš© ê¸ˆì§€.

ë§íˆ¬: ì¹œêµ¬í•œí…Œ ì–˜ê¸°í•˜ë“¯ì´ í¸í•˜ê²Œ. ì–µì§€ë¡œ ì¸í„°ë„· ìš©ì–´ ë„£ì§€ ë§ˆ.
ìì—°ìŠ¤ëŸ¬ìš°ë©´ "ã…‹ã…‹"ë‚˜ "ã„·ã„·" ì¨ë„ ë˜ì§€ë§Œ ë§¤ ë¬¸ì¥ë§ˆë‹¤ ì“°ì§€ ë§ˆ.

ì ˆëŒ€ ì“°ì§€ ë§ ê²ƒ:
{slop_words}

[ì£¼ì œ] {topic}

[ì°¸ê³  í—¤ë“œë¼ì¸]
{source_text}

JSONë§Œ ì¶œë ¥:
{{
  "title": "ì œëª© 15ì ì´ë‚´ (ì´ëª¨ì§€ ê¸ˆì§€)",
  "tts_script": "ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ëŒ€ë³¸",
  "tags": ["íƒœê·¸1", "íƒœê·¸2", "íƒœê·¸3", "íƒœê·¸4", "íƒœê·¸5"],
  "description": "ì„¤ëª…ë€ 2ì¤„"
}}"""

    def _build_prompt(self, topic: str, source_text: str) -> str:
        slop = ", ".join(Config.AI_SLOP_WORDS)
        has_real_source = bool(source_text and len(source_text) > 50
                               and "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ" not in source_text)

        if has_real_source:
            # ì›ê¸€ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ íŒ©íŠ¸ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸
            return self.PROMPT_WITH_SOURCE.format(
                topic=topic,
                source_text=source_text[:2000],
                slop_words=slop,
            )
        else:
            # ì›ê¸€ ì—†ìœ¼ë©´ ì •ë³´í˜• í”„ë¡¬í”„íŠ¸ (ë‰´ìŠ¤ í—¤ë“œë¼ì¸ë§Œ ì°¸ê³ )
            headlines = source_text if source_text else "ê´€ë ¨ í—¤ë“œë¼ì¸ ì—†ìŒ"
            return self.PROMPT_NO_SOURCE.format(
                topic=topic,
                source_text=headlines,
                slop_words=slop,
            )

    def _parse_json_response(self, text: str) -> dict:
        """Gemini JSON ì‘ë‹µ íŒŒì‹± (ë§ˆí¬ë‹¤ìš´ + ì œì–´ë¬¸ì ì²˜ë¦¬)."""
        text = re.sub(r"```json\s*", "", text)
        text = re.sub(r"```\s*", "", text)

        # JSON ê°ì²´ ì¶”ì¶œ (regex)
        match = re.search(r'\{.*\}', text, re.DOTALL)
        if match:
            json_str = match.group(0)
        else:
            json_str = text.strip()

        # ì œì–´ ë¬¸ì ì œê±°
        json_str = re.sub(r'[\x00-\x1f\x7f]', '', json_str)
        # ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
        json_str = json_str.replace('\n', '\\n')

        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}") from e

    def _call_gemini(self, topic: str, source_text: str) -> Optional[dict]:
        """Gemini 2.0 Flash - ë¬´ë£Œ, 1ìˆœìœ„"""
        try:
            import google.generativeai as genai

            api_key = os.getenv("GOOGLE_API_KEY")
            if not api_key:
                print("  [WARN] GOOGLE_API_KEY ì—†ìŒ")
                return None

            genai.configure(api_key=api_key)
            model = genai.GenerativeModel("gemini-2.0-flash")

            prompt = self._build_prompt(topic, source_text)

            response = model.generate_content(
                prompt,
                generation_config=genai.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=1024,
                ),
            )

            result = self._parse_json_response(response.text)
            print("  [OK] Gemini ëŒ€ë³¸ ìƒì„± ì„±ê³µ")
            return result

        except Exception as e:
            print(f"  [WARN] Gemini ì‹¤íŒ¨: {e}")
            return None

    # v5.0: OpenAI í´ë°± ì œê±° â€” Gemini 2.0 Flashë§Œ ì‚¬ìš© (ë¬´ë£Œ)

    def _quality_check(self, script_data: dict) -> int:
        """í’ˆì§ˆ ì±„ì  (100ì  ë§Œì , ê°ì  ë°©ì‹) â€” ì½˜í…ì¸  ë‹¤ì–‘ì„± + ì •ë³´ ë°€ë„ í¬í•¨"""
        score = 100
        reasons = []

        text = script_data.get("tts_script", "")
        title = script_data.get("title", "")

        # ê¸¸ì´ ì²´í¬ (250~350ì ìµœì )
        if len(text) < 150:
            score -= 30
            reasons.append(f"ë„ˆë¬´ ì§§ìŒ ({len(text)}ì)")
        elif len(text) < 250:
            score -= 10
            reasons.append(f"ì§§ìŒ ({len(text)}ì)")
        elif len(text) > 400:
            score -= 15
            reasons.append(f"ë„ˆë¬´ ê¹€ ({len(text)}ì)")

        # ë¬¸ì¥ ìˆ˜ ì²´í¬ (15~25ë¬¸ì¥ ìµœì )
        sentences = [s.strip() for s in text.split("\n") if s.strip()]
        if len(sentences) < 12:
            score -= 15
            reasons.append(f"ë¬¸ì¥ ë¶€ì¡± ({len(sentences)}ë¬¸ì¥)")
        elif len(sentences) > 30:
            score -= 10
            reasons.append(f"ë¬¸ì¥ ê³¼ë‹¤ ({len(sentences)}ë¬¸ì¥)")

        # ë¬¸ì¥ ê¸¸ì´ ë‹¤ì–‘ì„± ì²´í¬ (ì „ë¶€ ë¹„ìŠ·í•œ ê¸¸ì´ë©´ ê°ì )
        if sentences:
            lens = [len(s) for s in sentences]
            avg_len = sum(lens) / len(lens)
            variance = sum((l - avg_len) ** 2 for l in lens) / len(lens)
            if variance < 5:  # ë¶„ì‚°ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ = ì „ë¶€ ë¹„ìŠ·í•œ ê¸¸ì´
                score -= 10
                reasons.append("ë¬¸ì¥ ê¸¸ì´ê°€ ë„ˆë¬´ ê· ì¼ (ë¦¬ë“¬ê° ë¶€ì¡±)")

        # AI ìŠ¬ë¡­ ì²´í¬
        slop_found = 0
        for word in Config.AI_SLOP_WORDS:
            if word in text:
                slop_found += 1
        if slop_found:
            score -= min(slop_found * 10, 30)
            reasons.append(f"AIìŠ¬ë¡­ {slop_found}ê°œ")

        # ì‹¤ëª… ì²´í¬
        name_pattern = r"[ê¹€ì´ë°•ìµœì •ê°•ì¡°ìœ¤ì¥ì„][ê°€-í£]{1,2}(?:ì”¨|ë‹˜|ì´|ê°€|ì„|ë¥¼)"
        if re.search(name_pattern, text):
            score -= 20
            reasons.append("ì‹¤ëª… í¬í•¨ ì˜ì‹¬")

        # ê¸ˆì§€ì–´ ì²´í¬
        banned = ["ì—¬ëŸ¬ë¶„", "ê²½ì œí•™", "ë”œë ˆë§ˆ", "ë§ˆë¬´ë¦¬í•˜ë©°", "ì •ë¦¬í•˜ìë©´",
                  "ì•Œì•„ë³´ê² ", "ì‚´í´ë³´ê² ", "í¥ë¯¸ë¡­", "ë†€ë¼ìš´"]
        for bw in banned:
            if bw in text:
                score -= 15
                reasons.append(f"ê¸ˆì§€ì–´: '{bw}'")

        # ì œëª© ê¸¸ì´
        if len(title) > 20:
            score -= 10
            reasons.append(f"ì œëª© ë„ˆë¬´ ê¹€ ({len(title)}ì)")

        # ì´ëª¨ì§€ ì²´í¬
        if Config.EMOJI_PATTERN.search(title) or Config.EMOJI_PATTERN.search(text):
            score -= 10
            reasons.append("ì´ëª¨ì§€ í¬í•¨")

        # ìì—°ìŠ¤ëŸ¬ì›€ ì²´í¬ (êµ¬ì–´ì²´ í”ì ì´ ìµœì†Œí•œ ìˆëŠ”ì§€)
        natural_markers = ["?", "!", "...", "ê·¼ë°", "ì§„ì§œ", "ì¢€"]
        natural_count = sum(1 for m in natural_markers if m in text)
        if natural_count == 0:
            score -= 10
            reasons.append("ë„ˆë¬´ ë”±ë”±í•œ ë¬¸ì²´")

        # ë°˜ë³µ ë¬¸ì¥ ì²´í¬ (ë™ì¼ ì‹œì‘ ë¬¸ì¥ ê°ì )
        starts = [s[:5] for s in sentences if len(s) >= 5]
        if starts:
            from collections import Counter
            start_counts = Counter(starts)
            repeated = sum(1 for c in start_counts.values() if c > 2)
            if repeated:
                score -= repeated * 5
                reasons.append(f"ë°˜ë³µ íŒ¨í„´ {repeated}ê°œ")

        # â”€â”€ ì •í™•ì„± ê²€ì¦ (í—ˆìœ„ì •ë³´ íƒì§€) â”€â”€
        # 1) ë¯¸í™•ì¸ ì—°êµ¬/ì „ë¬¸ê°€ ì¸ìš© ê°ì 
        fake_authority = [
            "ì—°êµ¬ì— ë”°ë¥´ë©´", "ì—°êµ¬ê²°ê³¼", "ì—°êµ¬íŒ€", "ì—°êµ¬ì§„",
            "ì „ë¬¸ê°€ì— ë”°ë¥´ë©´", "ì „ë¬¸ê°€ë“¤ì€", "ê´€ê³„ìì— ë”°ë¥´ë©´",
            "í†µê³„ì— ë”°ë¥´ë©´", "ì¡°ì‚¬ì— ë”°ë¥´ë©´",
        ]
        fake_count = sum(1 for fa in fake_authority if fa in text)
        if fake_count:
            score -= fake_count * 15
            reasons.append(f"ë¯¸í™•ì¸ ì¸ìš© {fake_count}ê±´")

        # 2) ìœ„í—˜ ì •ë³´ íŒ¨í„´ (ì˜í•™/ë²•ë¥ /ê¸ˆìœµ)
        danger_patterns = [
            r'\d+%\s*(í™•ë¥ |ê°€ëŠ¥ì„±|ì¹˜ë£Œìœ¨|íš¨ê³¼|ê°ì†Œ)',
            r'(ë²Œê¸ˆ|ê³¼íƒœë£Œ|ì§•ì—­)\s*\d+',
            r'(íˆ¬ì|ìˆ˜ìµë¥ ?|ì´ììœ¨?)\s*\d+',
        ]
        for dp in danger_patterns:
            if re.search(dp, text):
                score -= 20
                reasons.append("ìœ„í—˜ì •ë³´ í¬í•¨(ì˜í•™/ë²•ë¥ /ê¸ˆìœµ)")
                break

        # 3) ê°€ì§œ ê²½í—˜ë‹´ íŒ¨í„´
        fake_exp = [
            "ë‚´ ì¹œêµ¬ê°€", "ë‚´ ë™ìƒì´", "ì˜†ì§‘ ì•„ì €ì”¨",
            "ì•„ëŠ” í˜•ì´", "ì§ì ‘ í•´ë´¤ëŠ”ë°",
        ]
        if not hasattr(self, '_source_text') or not self._source_text:
            # ì†ŒìŠ¤ ì—†ëŠ” ëŒ€ë³¸ì—ì„œ êµ¬ì²´ì  ê²½í—˜ë‹´ = í—ˆìœ„ ê°€ëŠ¥ì„± ë†’ìŒ
            exp_count = sum(1 for fe in fake_exp if fe in text)
            if exp_count:
                score -= exp_count * 10
                reasons.append(f"ë¯¸í™•ì¸ ê²½í—˜ë‹´ {exp_count}ê±´")

        score = max(0, score)

        if reasons:
            print(f"  í’ˆì§ˆ: {score}ì  (ê°ì : {', '.join(reasons)})")
        else:
            print(f"  í’ˆì§ˆ: {score}ì  - ì™„ë²½")

        return score

    def _post_process(self, script_data: dict) -> dict:
        """í›„ì²˜ë¦¬: ë³¼ë“œ ì œê±° + AI ìŠ¬ë¡­ êµì²´ + ê¸´ ë¬¸ì¥ ë¶„ë¦¬"""
        text = script_data.get("tts_script", "")

        # ë³¼ë“œ/ì´íƒ¤ë¦­ ë§ˆí¬ë‹¤ìš´ ì œê±°
        text = re.sub(r"\*\*(.+?)\*\*", r"\1", text)
        text = re.sub(r"\*(.+?)\*", r"\1", text)

        # AI ìŠ¬ë¡­ â†’ ì»¤ë®¤ë‹ˆí‹° ë§íˆ¬ë¡œ êµì²´ (ëŒ€í­ í™•ì¥)
        replacements = {
            "í¥ë¯¸ë¡­": "ì¬ë°Œ",
            "ë†€ë¼ìš´": "ëŒ€ë°•ì¸",
            "ì¶©ê²©ì ": "ë¯¸ì¹œ",
            "ì‹¬ì¸µ": "ì§„ì§œ",
            "íƒêµ¬": "íŒŒí—¤ì¹˜",
            "ì•Œì•„ë³´ê² ": "ì–˜ê¸°í•´ë³¼",
            "ì‚´í´ë³´ê² ": "ë´ë³¼",
            "ì£¼ëª©í•  ë§Œí•œ": "ê°œì©ŒëŠ”",
            "ëˆˆì—¬ê²¨ë³¼": "ë´ì•¼ í• ",
            "í•œí¸ìœ¼ë¡œëŠ”": "ê·¼ë°",
            "ë§¤ë ¥ì ì¸": "ëŒë¦¬ëŠ”",
            "ì¸ìƒì ì¸": "ì©ŒëŠ”",
            "ë…ë³´ì ì¸": "ê°œìœ ì¼í•œ",
            "í˜ì‹ ì ì¸": "ìƒˆë¡œìš´",
            "íšê¸°ì ì¸": "ëŒ€ë°•ì¸",
            "ê·¸ë¿ë§Œ ì•„ë‹ˆë¼": "ê²Œë‹¤ê°€",
            "ì´ì— ë”í•´": "ê·¸ë¦¬ê³ ",
            "ë‚˜ì•„ê°€": "ë”",
            "ë”ë¶ˆì–´": "ê·¸ë¦¬ê³ ",
            "ê²°ë¡ ì ìœ¼ë¡œ": "ê²°êµ­",
            "ì •ë¦¬í•˜ìë©´": "ê±",
            "ìš”ì•½í•˜ìë©´": "ê±",
            "ë˜ìƒˆê²¨ ë³´": "ìƒê°í•´ ë³´",
            "í° êµí›ˆ": "ë°°ìš´ ê±°",
            "í•˜ê² ìŠµë‹ˆë‹¤": "í• ê²Œ",
            "ë˜ê² ìŠµë‹ˆë‹¤": "ë  ê±°ì•¼",
            "ì „ë¬¸ê°€ë“¤ì€": "ì‚¬ëŒë“¤ì´",
            "ê·€ì¶”ê°€ ì£¼ëª©": "ê¸°ëŒ€ë¨",
        }
        for old, new in replacements.items():
            text = text.replace(old, new)

        # ì´ëª¨ì§€ ì œê±°
        text = Config.EMOJI_PATTERN.sub("", text)

        # ë¹ˆ ì¤„ ì •ë¦¬ë§Œ (ì¸ìœ„ì  ë¬¸ì¥ ë¶„ë¦¬ í•˜ì§€ ì•ŠìŒ)
        lines = [l.strip() for l in text.split("\n") if l.strip()]
        text = "\n".join(lines)
        script_data["tts_script"] = text

        # ì œëª©ì—ì„œë„ ì´ëª¨ì§€ ì œê±°
        title = script_data.get("title", "")
        title = Config.EMOJI_PATTERN.sub("", title).strip()
        script_data["title"] = title

        return script_data

    def generate(self, topic: str, source_text: str) -> dict:
        """ëŒ€ë³¸ ìƒì„± ë©”ì¸ - í’ˆì§ˆ 85ì  ì´ìƒê¹Œì§€ ìµœëŒ€ 3íšŒ ì¬ì‹œë„"""
        print("\n" + "=" * 60)
        print("STEP 2: ëŒ€ë³¸ ìƒì„± (Gemini 2.0 Flash â€” ë¬´ë£Œ)")
        print("=" * 60)

        # ì •í™•ì„± ê²€ì¦ìš© ì†ŒìŠ¤ ì €ì¥
        self._source_text = source_text

        if not source_text:
            print("  [WARN] ì›ê¸€ ë³¸ë¬¸ ì—†ìŒ â€” ì£¼ì œë§Œìœ¼ë¡œ ìƒì„±")

        best_result = None
        best_score = 0

        for attempt in range(Config.MAX_RETRY):
            print(f"\n  ì‹œë„ {attempt + 1}/{Config.MAX_RETRY}")

            result = self._call_gemini(topic, source_text)

            if result is None:
                print("  [ERROR] Gemini ì‹¤íŒ¨ â€” ì¬ì‹œë„")
                continue

            result = self._post_process(result)
            score = self._quality_check(result)

            if score > best_score:
                best_score = score
                best_result = result
                best_result["quality_score"] = score

            if score >= Config.MIN_QUALITY_SCORE:
                result["quality_score"] = score
                print(f"\n  [OK] ëŒ€ë³¸ í™•ì •! (ì ìˆ˜: {score})")
                print(f"  ì œëª©: {result.get('title', 'N/A')}")
                print(f"  ê¸¸ì´: {len(result.get('tts_script', ''))}ì")
                return result

            print(f"  [FAIL] {score}ì  < {Config.MIN_QUALITY_SCORE}ì  -> ì¬ìƒì„±")

        # graceful fallback
        if best_result and best_score > 0 and len(best_result.get("tts_script", "")) >= 100:
            print(f"\n  [WARN] {Config.MAX_RETRY}íšŒ ë¯¸ë‹¬ -> ìµœì„ ì˜ ê²°ê³¼ ì‚¬ìš© ({best_score}ì )")
            return best_result

        raise RuntimeError("ëŒ€ë³¸ ìƒì„± ì‹¤íŒ¨: 3íšŒ ëª¨ë‘ í’ˆì§ˆ ë¯¸ë‹¬")


# ============================================================================
# STEP 3: TTS ìƒì„± (edge-tts ì „ìš© â€” ë¬´ë£Œ, ë‹¨ì–´ë³„ íƒ€ì´ë°)
# ============================================================================
class TTSEngine:
    """
    v5.0: edge-tts ì „ìš© TTS (ë¬´ë£Œ, WordBoundary íƒ€ì´ë° ì§€ì›)
    ìœ ë£Œ í´ë°±(ElevenLabs/OpenAI) ì œê±° â€” API ë¹„ìš© 0ì›
    """

    async def _edge_tts(self, text: str, output_mp3: str) -> list[dict]:
        """edge-ttsë¡œ ìŒì„± ìƒì„± + ë‹¨ì–´ë³„ íƒ€ì´ë° ìˆ˜ì§‘"""
        import edge_tts

        word_timings = []
        communicate = edge_tts.Communicate(
            text=text,
            voice=Config.TTS_VOICE,
            rate=Config.TTS_RATE,
            pitch=Config.TTS_PITCH,
        )

        with open(output_mp3, "wb") as f:
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    f.write(chunk["data"])
                elif chunk["type"] == "WordBoundary":
                    word_timings.append({
                        "word": chunk["text"],
                        "start_ms": chunk["offset"] // 10000,
                        "end_ms": (chunk["offset"] + chunk["duration"]) // 10000,
                    })

        return word_timings

    # v5.0: ElevenLabs / OpenAI TTS ì œê±° â€” edge-ttsë§Œ ì‚¬ìš© (ë¬´ë£Œ)

    async def generate_with_timing(self, text: str, output_mp3: str) -> list[dict]:
        """edge-tts ì „ìš© TTS (ë¬´ë£Œ, ë‹¨ì–´ë³„ íƒ€ì´ë° ì§€ì›)"""
        try:
            word_timings = await self._edge_tts(text, output_mp3)
            if os.path.exists(output_mp3) and os.path.getsize(output_mp3) > 1000:
                print(f"  [OK] edge-tts ì„±ê³µ: {len(word_timings)}ê°œ íƒ€ì´ë°")
                return word_timings
            raise RuntimeError("edge-tts ì¶œë ¥ íŒŒì¼ ì—†ìŒ ë˜ëŠ” ë¹„ì •ìƒ")
        except Exception as e:
            print(f"  [ERROR] edge-tts ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"TTS ì‹¤íŒ¨ (edge-tts): {e}")

    def generate(self, text: str, output_mp3: str) -> list[dict]:
        """ë™ê¸° ë˜í¼"""
        print("\n" + "=" * 60)
        print("STEP 3: TTS ìƒì„± (edge-tts â€” ë¬´ë£Œ)")
        print("=" * 60)
        return asyncio.run(self.generate_with_timing(text, output_mp3))


# ============================================================================
# STEP 3.5: ì˜¤ë””ì˜¤ ë§ˆìŠ¤í„°ë§ (2-pass loudnorm, -14 LUFS)
# ============================================================================
def master_audio(input_path: str, output_path: str) -> str:
    """ì˜¤ë””ì˜¤ ë³¼ë¥¨ ì •ê·œí™” + EQ. 2-pass loudnorm."""
    ffmpeg = _find_ffmpeg_exe()
    try:
        measure_cmd = [
            ffmpeg, "-i", input_path,
            "-af", "loudnorm=I=-14:TP=-1.5:LRA=11:print_format=json",
            "-f", "null", "-",
        ]
        result = subprocess.run(measure_cmd, capture_output=True, timeout=60)
        stderr_text = result.stderr.decode("utf-8", errors="ignore")

        json_matches = list(
            re.finditer(r'\{[^{}]*"input_i"[^{}]*\}', stderr_text, re.DOTALL)
        )
        if not json_matches:
            raise ValueError("loudnorm JSON íŒŒì‹± ì‹¤íŒ¨")

        measured = json.loads(json_matches[-1].group(0))
        m_I = measured.get("input_i", "-14.0")
        m_TP = measured.get("input_tp", "-1.5")
        m_LRA = measured.get("input_lra", "11.0")
        m_thresh = measured.get("input_thresh", "-24.0")

        normalize_cmd = [
            ffmpeg, "-y", "-i", input_path,
            "-af", (
                "highpass=f=80,"
                "acompressor=threshold=-20dB:ratio=4:attack=5:release=50,"
                f"loudnorm=I=-14:TP=-1.5:LRA=11:"
                f"measured_I={m_I}:measured_TP={m_TP}:"
                f"measured_LRA={m_LRA}:measured_thresh={m_thresh}:"
                f"linear=true"
            ),
            "-ar", "44100", "-ac", "1",
            output_path,
        ]
        subprocess.run(normalize_cmd, capture_output=True, check=True, timeout=120)
        print("  [OK] ë§ˆìŠ¤í„°ë§ ì™„ë£Œ (2-pass, -14 LUFS)")
        return output_path

    except Exception as e:
        try:
            fallback_cmd = [
                ffmpeg, "-y", "-i", input_path,
                "-af", "loudnorm=I=-14:TP=-1.5:LRA=11",
                "-ar", "44100", "-ac", "1",
                output_path,
            ]
            subprocess.run(fallback_cmd, capture_output=True, check=True, timeout=120)
            print("  [OK] ë§ˆìŠ¤í„°ë§ ì™„ë£Œ (1-pass í´ë°±)")
            return output_path
        except Exception as e:
            logger.warning(f"ë§ˆìŠ¤í„°ë§ í´ë°± ì‹¤íŒ¨: {e}")
            print(f"  [WARN] ë§ˆìŠ¤í„°ë§ ì‹¤íŒ¨ - ì›ë³¸ ì‚¬ìš©: {e}")
            return input_path


def adjust_audio_speed(audio_path: str, speed_factor: float, output_path: str) -> str:
    """FFmpeg atempoë¡œ ì˜¤ë””ì˜¤ ì†ë„ ì¡°ì ˆ."""
    speed_factor = max(0.8, min(1.25, speed_factor))
    if abs(speed_factor - 1.0) < 0.01:
        return audio_path

    ffmpeg = _find_ffmpeg_exe()
    try:
        cmd = [
            ffmpeg, "-y", "-i", audio_path,
            "-filter:a", f"atempo={speed_factor:.4f}",
            "-vn", output_path,
        ]
        subprocess.run(cmd, capture_output=True, check=True, timeout=60)
        print(f"  [OK] ì†ë„ ì¡°ì ˆ: x{speed_factor:.2f}")
        return output_path
    except Exception as e:
        print(f"  [WARN] ì†ë„ ì¡°ì ˆ ì‹¤íŒ¨: {e}")
        return audio_path


# ============================================================================
# STEP 4: ë‹¨ì–´ë³„ í•˜ì´ë¼ì´íŠ¸ ìë§‰ (ASS í˜•ì‹)
# ============================================================================
class SubtitleGenerator:
    """ASS í˜•ì‹ ë‹¨ì–´ë³„ í•˜ì´ë¼ì´íŠ¸ (í˜„ì¬=ë…¸ë€, ë‚˜ë¨¸ì§€=í°)"""

    def _group_words_into_lines(
        self, word_timings: list[dict], max_chars: int = 15
    ) -> list[dict]:
        lines = []
        current_line = []
        current_chars = 0

        for wt in word_timings:
            word = wt["word"].strip()
            if not word:
                continue

            if current_chars + len(word) > max_chars and current_line:
                lines.append({
                    "words": current_line.copy(),
                    "start_ms": current_line[0]["start_ms"],
                    "end_ms": current_line[-1]["end_ms"],
                })
                current_line = []
                current_chars = 0

            current_line.append(wt)
            current_chars += len(word) + 1

        if current_line:
            lines.append({
                "words": current_line,
                "start_ms": current_line[0]["start_ms"],
                "end_ms": current_line[-1]["end_ms"],
            })

        return lines

    def _ms_to_ass_time(self, ms: int) -> str:
        h = ms // 3600000
        m = (ms % 3600000) // 60000
        s = (ms % 60000) // 1000
        cs = (ms % 1000) // 10
        return f"{h}:{m:02d}:{s:02d}.{cs:02d}"

    def _get_ass_header(self) -> str:
        """ASS header"""
        return f"""[Script Info]
Title: youshorts subtitles
ScriptType: v4.00+
PlayResX: {Config.WIDTH}
PlayResY: {Config.HEIGHT}
WrapStyle: 0

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,{Config.SUBTITLE_FONT},{Config.SUBTITLE_SIZE},{Config.SUBTITLE_COLOR_NORMAL},&H000000FF,&H00000000,&H80000000,-1,0,0,0,100,100,0,0,1,{Config.SUBTITLE_OUTLINE},{Config.SUBTITLE_SHADOW},2,40,40,{Config.SUBTITLE_MARGIN_V},1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""


    def generate_ass(self, word_timings: list[dict], output_ass: str) -> str:
        """ASS ìë§‰ íŒŒì¼ ìƒì„± - ë‹¨ì–´ë³„ í•˜ì´ë¼ì´íŠ¸"""
        lines = self._group_words_into_lines(word_timings)

        ass_content = self._get_ass_header()

        for line in lines:
            words_in_line = line["words"]
            for wi, current_word in enumerate(words_in_line):
                start = self._ms_to_ass_time(current_word["start_ms"])
                if wi + 1 < len(words_in_line):
                    end = self._ms_to_ass_time(words_in_line[wi + 1]["start_ms"])
                else:
                    end = self._ms_to_ass_time(current_word["end_ms"])

                text_parts = []
                for wj, w in enumerate(words_in_line):
                    word_text = w["word"].strip()
                    if not word_text:
                        continue
                    if wj == wi:
                        text_parts.append(
                            f"{{\\c{Config.SUBTITLE_COLOR_HIGHLIGHT}"
                            f"\\b1}}{word_text}{{\\r}}"
                        )
                    else:
                        text_parts.append(
                            f"{{\\c{Config.SUBTITLE_COLOR_NORMAL}"
                            f"}}{word_text}{{\\r}}"
                        )

                dialogue_text = " ".join(text_parts)
                ass_content += (
                    f"Dialogue: 0,{start},{end},Default,,0,0,0,,"
                    f"{dialogue_text}\n"
                )

        with open(output_ass, "w", encoding="utf-8") as f:
            f.write(ass_content)

        print(f"  [OK] ASS ìë§‰ ìƒì„±: {output_ass}")
        print(f"  {len(lines)}ê°œ ì¤„, ë‹¨ì–´ë³„ í•˜ì´ë¼ì´íŠ¸ ì ìš©")
        return output_ass

    def generate_ass_from_chunks(
        self, text: str, total_duration: float, output_ass: str
    ) -> str:
        """WordBoundary ì—†ì„ ë•Œ ì²­í¬ ê¸°ë°˜ í´ë°± ìë§‰"""
        sentences = re.split(r'(?<=[.?!~])\s*|(?<=ã…‹)\s+|(?<=ã…)\s+', text)
        chunks = [s.strip() for s in sentences if s.strip()]

        if not chunks:
            chunks = [text[i:i+15] for i in range(0, len(text), 15)]

        chunk_duration = total_duration / len(chunks) if chunks else 3.0

        ass_content = self._get_ass_header()

        for i, chunk in enumerate(chunks):
            start_ms = int(i * chunk_duration * 1000)
            end_ms = int((i + 1) * chunk_duration * 1000)
            start = self._ms_to_ass_time(start_ms)
            end = self._ms_to_ass_time(end_ms)

            highlighted = (
                f"{{\\c{Config.SUBTITLE_COLOR_HIGHLIGHT}\\b1}}"
                f"{chunk}{{\\r}}"
            )

            if i + 1 < len(chunks):
                next_text = (
                    f"\\N{{\\c{Config.SUBTITLE_COLOR_NORMAL}}}"
                    f"{chunks[i+1]}{{\\r}}"
                )
            else:
                next_text = ""

            ass_content += (
                f"Dialogue: 0,{start},{end},Default,,0,0,0,,"
                f"{highlighted}{next_text}\n"
            )

        with open(output_ass, "w", encoding="utf-8") as f:
            f.write(ass_content)

        print(f"  [OK] ASS í•˜ì´ë¼ì´íŠ¸ ìë§‰: {output_ass}")
        print(f"  {len(chunks)}ê°œ ì²­í¬, í˜„ì¬=ë…¸ë€/ë‹¤ìŒ=í°ìƒ‰")
        return output_ass


# ============================================================================
# STEP 5: ë°°ê²½ ì˜ìƒ + BGM + ë Œë”ë§
# ============================================================================
class VideoRenderer:
    """FFmpeg CLIë¡œ ë°°ê²½+TTS+BGM+ìë§‰ í•©ì„±"""

    def _find_ffmpeg(self) -> str:
        return _find_ffmpeg_exe()

    def _find_ffprobe(self) -> str:
        return _find_ffprobe_exe()

    def _resolve_bg_mode(self, topic: str) -> str:
        """ì£¼ì œ í‚¤ì›Œë“œ â†’ ë°°ê²½ ëª¨ë“œ ê²°ì • (gameplay / gradient)"""
        for mode, keywords in Config.TOPIC_BG_MAP.items():
            if any(kw in topic for kw in keywords):
                return mode
        return "gradient"  # ê¸°ë³¸ê°’

    def _resolve_gradient_colors(self, topic: str) -> tuple[str, str]:
        """ì£¼ì œ í‚¤ì›Œë“œ â†’ ê·¸ë¼ë””ì–¸íŠ¸ ìƒ‰ìƒ ì„ íƒ"""
        for category, keywords in Config.GRADIENT_TOPIC_MAP.items():
            if any(kw in topic for kw in keywords):
                return Config.GRADIENT_COLORS[category]
        return Config.GRADIENT_COLORS["default"]

    @staticmethod
    def _hex_to_rgb(hex_color: str) -> tuple[int, int, int]:
        """#RRGGBB â†’ (R, G, B)"""
        h = hex_color.lstrip("#")
        return int(h[0:2], 16), int(h[2:4], 16), int(h[4:6], 16)

    def _generate_gradient_bg(self, topic: str) -> Optional[str]:
        """FFmpeg geq í•„í„°ë¡œ ì‹¤ì œ ì„¸ë¡œ ê·¸ë¼ë””ì–¸íŠ¸ ë°°ê²½ ì˜ìƒ ìƒì„± (65ì´ˆ)"""
        c0, c1 = self._resolve_gradient_colors(topic)
        r0, g0, b0 = self._hex_to_rgb(c0)
        r1, g1, b1 = self._hex_to_rgb(c1)

        temp_dir = Config.BASE_DIR / "temp"
        temp_dir.mkdir(exist_ok=True)
        gen_mp4 = str(temp_dir / "gradient_bg.mp4")
        ffmpeg = self._find_ffmpeg()
        dur = 65
        W, H, FPS = Config.WIDTH, Config.HEIGHT, Config.FPS

        # geq í•„í„°: ìƒë‹¨(c0) â†’ í•˜ë‹¨(c1) ì„¸ë¡œ ê·¸ë¼ë””ì–¸íŠ¸ + ë…¸ì´ì¦ˆë¡œ ë¯¸ì„¸ ì›€ì§ì„
        # Yì¢Œí‘œ(0=ìƒë‹¨, H=í•˜ë‹¨) ê¸°ë°˜ ì„ í˜• ë³´ê°„
        geq_r = f"{r0}+(({r1}-{r0})*Y/{H})"
        geq_g = f"{g0}+(({g1}-{g0})*Y/{H})"
        geq_b = f"{b0}+(({b1}-{b0})*Y/{H})"

        lavfi = (
            f"color=c=black:s={W}x{H}:r={FPS}:d={dur},"
            f"geq=r='{geq_r}':g='{geq_g}':b='{geq_b}',"
            f"noise=alls=15:allf=t+u"
        )

        try:
            cmd = [
                ffmpeg, "-y",
                "-f", "lavfi", "-i", lavfi,
                "-t", str(dur),
                "-c:v", "libx264", "-preset", "ultrafast", "-crf", "26",
                "-pix_fmt", "yuv420p",
                gen_mp4,
            ]
            result = subprocess.run(cmd, capture_output=True, timeout=120)
            if result.returncode == 0 and os.path.exists(gen_mp4):
                size_mb = os.path.getsize(gen_mp4) / (1024 * 1024)
                print(f"  [OK] ê·¸ë¼ë””ì–¸íŠ¸ ë°°ê²½ ìƒì„± ({c0}â†’{c1}, {size_mb:.1f}MB)")
                return gen_mp4
            else:
                stderr = result.stderr.decode("utf-8", errors="ignore")[-300:]
                print(f"  [WARN] ê·¸ë¼ë””ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {stderr[:200]}")
        except Exception as e:
            print(f"  [WARN] ê·¸ë¼ë””ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

    def _get_background_video(self, topic: str = "") -> Optional[str]:
        """ì£¼ì œ ê¸°ë°˜ ë°°ê²½ ì„ íƒ: gameplayâ†’ì‹¤ì œì˜ìƒ / gradientâ†’ìë™ìƒì„±"""
        bg_mode = self._resolve_bg_mode(topic)
        print(f"  ë°°ê²½ ëª¨ë“œ: {bg_mode} (ì£¼ì œ: {topic[:30]})")

        # gameplay ëª¨ë“œ: ì‹¤ì œ ë°°ê²½ ì˜ìƒ ì‚¬ìš©
        if bg_mode == "gameplay":
            bg_dir = Config.BG_DIR
            videos = []
            if bg_dir.exists():
                videos = list(bg_dir.rglob("*.mp4"))
            if videos:
                selected = random.choice(videos)
                print(f"  ë°°ê²½ ì„ íƒ: {selected.name}")
                return str(selected)
            print("  [INFO] gameplay ì˜ìƒ ì—†ìŒ â†’ ê·¸ë¼ë””ì–¸íŠ¸ í´ë°±")

        # gradient ëª¨ë“œ (ë˜ëŠ” gameplay í´ë°±)
        result = self._generate_gradient_bg(topic)
        if result:
            return result

        # ìµœì¢… í´ë°±: None â†’ render()ì—ì„œ ê²€ì • ë‹¨ìƒ‰
        return None

    def _get_random_bgm(self) -> Optional[str]:
        bgm_dir = Config.BGM_DIR
        if not bgm_dir.exists():
            return None
        bgms = list(bgm_dir.glob("*.mp3"))
        if not bgms:
            return None
        return str(random.choice(bgms))

    def _get_video_duration(self, video_path: str) -> float:
        # 1ì°¨: ffprobe
        try:
            result = subprocess.run(
                [self._find_ffprobe(), "-v", "quiet",
                 "-show_entries", "format=duration",
                 "-of", "csv=p=0", video_path],
                capture_output=True, text=True, timeout=10,
            )
            val = result.stdout.strip()
            if val:
                return float(val)
        except (ValueError, subprocess.SubprocessError, OSError) as e:
            logger.warning(f"ffprobe ë“€ë ˆì´ì…˜ í™•ì¸ ì‹¤íŒ¨: {e}")
        # 2ì°¨: ffmpeg -i stderrì—ì„œ Duration íŒŒì‹±
        try:
            result = subprocess.run(
                [self._find_ffmpeg(), "-i", video_path],
                capture_output=True, text=True, timeout=10,
                encoding="utf-8", errors="replace",
            )
            m = re.search(r"Duration:\s*(\d+):(\d+):(\d+)\.(\d+)", result.stderr)
            if m:
                h, mi, s, cs = int(m.group(1)), int(m.group(2)), int(m.group(3)), int(m.group(4))
                return h * 3600 + mi * 60 + s + cs / 100
        except (ValueError, subprocess.SubprocessError, OSError) as e:
            logger.warning(f"ffmpeg ë“€ë ˆì´ì…˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return 60  # ì•ˆì „í•œ ê¸°ë³¸ê°’

    def render(self, tts_mp3: str, ass_subtitle: str, output_mp4: str,
               topic: str = "", title: str = "") -> str:
        print("\n" + "=" * 60)
        print("STEP 5: FFmpeg ë Œë”ë§")
        print("=" * 60)

        ffmpeg = self._find_ffmpeg()
        bg_video = self._get_background_video(topic=topic)
        bgm_mp3 = self._get_random_bgm()

        tts_duration = self._get_video_duration(tts_mp3)
        target_duration = min(tts_duration + 1.5, Config.MAX_DURATION)

        # ASS íŒŒì¼ì„ ìƒëŒ€ê²½ë¡œ temp/sub.assì— ë³µì‚¬ (Windows í˜¸í™˜)
        import shutil
        temp_dir = Config.BASE_DIR / "temp"
        temp_dir.mkdir(exist_ok=True)
        temp_ass = str(temp_dir / "sub.ass")
        shutil.copy2(ass_subtitle, temp_ass)

        cmd = [ffmpeg, "-y"]

        if bg_video:
            bg_duration = self._get_video_duration(bg_video)
            # ìƒì„±ëœ ë°°ê²½(65ì´ˆ)ì´ë©´ duration ì‹ ë¢° ë¶ˆê°€ â†’ 0ë¶€í„° ì‹œì‘
            if bg_duration > target_duration + 10:
                max_start = max(0, bg_duration - target_duration - 5)
                random_start = random.uniform(0, max_start)
            else:
                random_start = 0
            cmd.extend(["-ss", f"{random_start:.1f}", "-i", bg_video])
            print(f"  ë Œë”ë§ ì‹œì‘...")
            print(f"  ëª©í‘œ ê¸¸ì´: {target_duration:.1f}ì´ˆ")
            print(f"  ë°°ê²½ ëœë¤ ì‹œì‘: {random_start:.1f}ì´ˆ")
        else:
            cmd.extend([
                "-f", "lavfi", "-i",
                f"color=c=black:s={Config.WIDTH}x{Config.HEIGHT}:"
                f"r={Config.FPS}:d={target_duration}",
            ])

        cmd.extend(["-i", tts_mp3])

        input_idx_bgm = None
        if bgm_mp3:
            cmd.extend(["-i", bgm_mp3])
            input_idx_bgm = 2

        video_filters = [
            "crop=ih*9/16:ih:(iw-ih*9/16)/2:0",
            f"scale={Config.WIDTH}:{Config.HEIGHT}",
        ]

        if os.path.exists(temp_ass):
            video_filters.append("ass=temp/sub.ass")

        # â”€â”€ ìƒë‹¨ ì œëª© ë°” (drawbox + drawtext via textfile) â”€â”€
        display_title = (title or topic or "")[:25]
        if display_title:
            # í•œê¸€ â†’ textfile ë°©ì‹ (FFmpeg text= í•œê¸€ ê¹¨ì§ ë°©ì§€)
            title_txt = str(temp_dir / "title.txt")
            with open(title_txt, "w", encoding="utf-8") as tf:
                tf.write(display_title)
            # Windows ê²½ë¡œ â†’ FFmpeg í˜¸í™˜ (C\:/path)
            fontpath = "C\\:/Windows/Fonts/malgunbd.ttf"
            titlepath = title_txt.replace("\\", "/").replace("C:/", "C\\:/")
            video_filters.append(
                "drawbox=x=0:y=60:w=iw:h=130:color=black@0.5:t=fill"
            )
            video_filters.append(
                f"drawtext=fontfile='{fontpath}'"
                f":textfile='{titlepath}'"
                f":fontcolor=white:fontsize=30"
                f":x=(w-text_w)/2:y=105"
            )
            print(f"  ìƒë‹¨ ì œëª© ë°”: '{display_title}'")

        video_filter_str = ",".join(video_filters)

        if input_idx_bgm is not None:
            audio_filter = (
                f"[{input_idx_bgm}:a]volume=0.15,aloop=loop=-1:size=2e+09[bgm];"
                f"[1:a][bgm]amix=inputs=2:duration=first:dropout_transition=2[aout]"
            )
            cmd.extend([
                "-filter_complex",
                f"[0:v]{video_filter_str}[vout];{audio_filter}",
                "-map", "[vout]", "-map", "[aout]",
            ])
        else:
            cmd.extend([
                "-vf", video_filter_str,
                "-map", "0:v", "-map", "1:a",
            ])

        cmd.extend([
            "-c:v", "libx264", "-preset", "fast", "-crf", "23",
            "-c:a", "aac", "-b:a", "192k",
            "-t", f"{target_duration:.1f}", "-shortest",
            output_mp4,
        ])

        result = subprocess.run(cmd, capture_output=True, timeout=300)

        if result.returncode == 0 and os.path.exists(output_mp4):
            size_mb = os.path.getsize(output_mp4) / (1024 * 1024)
            print(f"  [OK] ë Œë”ë§ ì™„ë£Œ: {output_mp4}")
            print(f"  íŒŒì¼ í¬ê¸°: {size_mb:.1f}MB")
            return output_mp4
        else:
            stderr = result.stderr.decode("utf-8", errors="ignore")[-500:]
            print(f"  [ERROR] ë Œë”ë§ ì‹¤íŒ¨:\n  {stderr}")
            raise RuntimeError("FFmpeg ë Œë”ë§ ì‹¤íŒ¨")


# ============================================================================
# STEP 6: ì´ë ¥ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
# ============================================================================
class HistoryManager:
    """history.jsonìœ¼ë¡œ ì¤‘ë³µ ë°©ì§€ (Jaccard ìœ ì‚¬ë„)"""

    def __init__(self):
        self.history_file = Config.HISTORY_FILE
        self._ensure_file()

    def _ensure_file(self):
        self.history_file.parent.mkdir(parents=True, exist_ok=True)
        if not self.history_file.exists():
            self.history_file.write_text("[]", encoding="utf-8")

    def _load(self) -> list[dict]:
        try:
            return json.loads(self.history_file.read_text(encoding="utf-8"))
        except (json.JSONDecodeError, OSError) as e:
            logger.warning(f"íˆìŠ¤í† ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    def is_duplicate(self, topic: str) -> bool:
        history = self._load()
        for h in history:
            prev_topic = h.get("topic", "")
            set_a = set(topic)
            set_b = set(prev_topic)
            if not set_a or not set_b:
                continue
            overlap = len(set_a & set_b) / len(set_a | set_b)
            if overlap > 0.8:
                return True
        return False

    def save(self, topic: str, title: str, output_file: str):
        history = self._load()
        history.append({
            "topic": topic,
            "title": title,
            "file": output_file,
            "date": datetime.now().isoformat(),
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        })
        self.history_file.write_text(
            json.dumps(history, ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        print(f"  [OK] ì´ë ¥ ì €ì¥ ì™„ë£Œ (ì´ {len(history)}ê±´)")


# ============================================================================
# STEP 7: ë©”íƒ€ë°ì´í„° ìƒì„± (í•´ì‹œíƒœê·¸ + SEO íƒœê·¸)
# ============================================================================
class MetadataGenerator:
    BASE_HASHTAGS = ["#shorts", "#ì‡¼ì¸ ", "#ìˆì¸ "]

    @staticmethod
    def generate_hashtags(script_data: dict) -> list[str]:
        tags = list(MetadataGenerator.BASE_HASHTAGS)
        for tag in script_data.get("tags", []):
            clean = tag.strip().replace(" ", "")
            if not clean.startswith("#"):
                clean = f"#{clean}"
            if clean not in tags:
                tags.append(clean)
        title = script_data.get("title", "")
        for word in re.findall(r'[ê°€-í£a-zA-Z]{2,}', title):
            t = f"#{word}"
            if t not in tags and len(tags) < 15:
                tags.append(t)
        return tags[:15]

    @staticmethod
    def generate_seo_tags(script_data: dict) -> list[str]:
        seo_tags = ["ìˆì¸ ", "ì‡¼ì¸ ", "shorts", "í•œêµ­", "ì‹¤í™”"]
        for tag in script_data.get("tags", []):
            clean = tag.strip().lstrip("#")
            if clean and clean not in seo_tags:
                seo_tags.append(clean)
        title = script_data.get("title", "")
        for word in re.findall(r'[ê°€-í£]{2,}', title):
            if word not in seo_tags and len(seo_tags) < 20:
                seo_tags.append(word)
        return seo_tags[:20]

    @staticmethod
    def generate(script_data: dict) -> dict:
        title = script_data.get("title", "ì‡¼ì¸ ")[:46]
        if "#Shorts" not in title:
            title = f"{title} #Shorts"
        hashtags = MetadataGenerator.generate_hashtags(script_data)
        seo_tags = MetadataGenerator.generate_seo_tags(script_data)
        desc_parts = [
            script_data.get("title", ""), "",
            " ".join(hashtags), "", "---",
            "ì´ ì˜ìƒì€ AI ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ì œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.",
        ]
        return {
            "title": title,
            "description": "\n".join(desc_parts),
            "tags": seo_tags,
            "hashtags": hashtags,
            "category": "22",
        }


# ============================================================================
# STEP 8: YouTube ì—…ë¡œë“œ (OAuth2)
# ============================================================================
class YouTubeUploader:
    """YouTube Shorts ì—…ë¡œë” â€” OAuth2 ì¸ì¦ + 3íšŒ ì¬ì‹œë„"""

    SCOPES = ["https://www.googleapis.com/auth/youtube.upload"]
    MAX_RETRIES = 3

    def __init__(self):
        self.client_id = os.getenv("YOUTUBE_CLIENT_ID", "")
        self.client_secret = os.getenv("YOUTUBE_CLIENT_SECRET", "")
        self.token_path = str(Config.BASE_DIR / "data" / "youtube_token.json")
        self.service = None
        self.available = bool(self.client_id and self.client_secret)

    def authenticate(self) -> bool:
        if not self.available:
            print("  [WARN] YOUTUBE_CLIENT_ID/SECRET ë¯¸ì„¤ì • -> ì—…ë¡œë“œ ë¶ˆê°€")
            return False

        try:
            from google.oauth2.credentials import Credentials
            from google_auth_oauthlib.flow import InstalledAppFlow
            from google.auth.transport.requests import Request
            from googleapiclient.discovery import build

            creds = None
            if os.path.exists(self.token_path):
                creds = Credentials.from_authorized_user_file(self.token_path, self.SCOPES)

            if creds and creds.expired and creds.refresh_token:
                try:
                    creds.refresh(Request())
                except Exception as e:
                    logger.warning(f"YouTube í† í° ê°±ì‹  ì‹¤íŒ¨: {e}")
                    creds = None

            if not creds or not creds.valid:
                flow = InstalledAppFlow.from_client_config(
                    {"installed": {
                        "client_id": self.client_id,
                        "client_secret": self.client_secret,
                        "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                        "token_uri": "https://oauth2.googleapis.com/token",
                        "redirect_uris": ["http://localhost"],
                    }},
                    self.SCOPES,
                )
                creds = flow.run_local_server(port=0)
                os.makedirs(os.path.dirname(self.token_path), exist_ok=True)
                with open(self.token_path, "w") as f:
                    f.write(creds.to_json())

            self.service = build("youtube", "v3", credentials=creds)
            return True

        except ImportError:
            print("  [WARN] google-auth-oauthlib ë¯¸ì„¤ì¹˜")
            return False
        except Exception as e:
            print(f"  [ERROR] YouTube ì¸ì¦ ì‹¤íŒ¨: {e}")
            return False

    def _do_upload(self, mp4_path: str, body: dict) -> Optional[str]:
        title = body["snippet"]["title"]
        for attempt in range(1, self.MAX_RETRIES + 1):
            try:
                from googleapiclient.http import MediaFileUpload
                media = MediaFileUpload(
                    mp4_path, mimetype="video/mp4", resumable=True,
                    chunksize=10 * 1024 * 1024,
                )
                request = self.service.videos().insert(
                    part="snippet,status", body=body, media_body=media,
                )
                print(f"  ì—…ë¡œë“œ ì‹œì‘: '{title}'")
                response = None
                while response is None:
                    status, response = request.next_chunk()
                    if status:
                        print(f"  ì—…ë¡œë“œ: {int(status.progress() * 100)}%")
                video_id = response["id"]
                url = f"https://youtube.com/shorts/{video_id}"
                print(f"  [OK] ì—…ë¡œë“œ ì™„ë£Œ: {url}")
                return url
            except Exception as e:
                print(f"  [WARN] ì—…ë¡œë“œ ì‹¤íŒ¨ ({attempt}/{self.MAX_RETRIES}): {e}")
                if attempt < self.MAX_RETRIES:
                    time.sleep(10)
        return None

    def upload(self, mp4_path: str, metadata: dict, privacy: str = "public") -> Optional[str]:
        if not self.service and not self.authenticate():
            return None
        title = metadata.get("title", "ì‡¼ì¸ ")[:100]
        body = {
            "snippet": {
                "title": title,
                "description": metadata.get("description", ""),
                "tags": metadata.get("tags", [])[:20],
                "categoryId": "22",
                "defaultLanguage": "ko",
            },
            "status": {
                "privacyStatus": privacy,
                "selfDeclaredMadeForKids": False,
            },
        }
        return self._do_upload(mp4_path, body)

    def upload_scheduled(self, mp4_path: str, metadata: dict,
                         video_index: int = 0) -> Optional[str]:
        if not self.service and not self.authenticate():
            return None
        tomorrow_9am = (
            datetime.utcnow().replace(hour=9, minute=0, second=0, microsecond=0)
            + timedelta(days=1)
        )
        publish_time = tomorrow_9am + timedelta(hours=4 * video_index)
        publish_at = publish_time.strftime("%Y-%m-%dT%H:%M:%S.000Z")
        print(f"  ì˜ˆì•½ ê³µê°œ: {publish_at} (ì˜ìƒ #{video_index + 1})")
        title = metadata.get("title", "ì‡¼ì¸ ")[:100]
        body = {
            "snippet": {
                "title": title,
                "description": metadata.get("description", ""),
                "tags": metadata.get("tags", [])[:20],
                "categoryId": "22",
                "defaultLanguage": "ko",
            },
            "status": {
                "privacyStatus": "private",
                "publishAt": publish_at,
                "selfDeclaredMadeForKids": False,
            },
        }
        return self._do_upload(mp4_path, body)


# ============================================================================
# íŒŒì¼ ì •ë¦¬ ìœ í‹¸
# ============================================================================
class FileCleaner:
    MIN_MP4_SIZE = 5 * 1024 * 1024

    @staticmethod
    def clean_output(output_dir: str = None):
        if output_dir is None:
            output_dir = str(Config.OUTPUT_DIR)
        cleaned = 0
        for root, dirs, files in os.walk(output_dir):
            for f in files:
                fp = os.path.join(root, f)
                if f.endswith(".mp4") and os.path.getsize(fp) < FileCleaner.MIN_MP4_SIZE:
                    os.remove(fp)
                    cleaned += 1
                    print(f"  ì‚­ì œ (ë¹„ì •ìƒ): {f}")
        temp_dir = os.path.join(str(Config.BASE_DIR), "temp")
        if os.path.exists(temp_dir):
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
        if cleaned:
            print(f"  [OK] {cleaned}ê°œ ë¹„ì •ìƒ íŒŒì¼ ì •ë¦¬")


# ============================================================================
# APIFY í¬ë¡¤ë§ ê°•í™”
# ============================================================================
class ApifyCrawler:
    SITES = [
        ("ì—í¨ì½”ë¦¬ì•„", "https://www.fmkorea.com/index.php?mid=humor_best"),
        ("ì¸ìŠ¤í‹°ì¦ˆ", "https://www.instiz.net/pt"),
        ("ë”ì¿ ", "https://theqoo.net/hot"),
        ("í´ë¦¬ì•™", "https://www.clien.net/service/board/park"),
        ("ë£¨ë¦¬ì›¹", "https://bbs.ruliweb.com/community/board/300143/best"),
    ]

    @staticmethod
    def crawl() -> list[dict]:
        import requests
        token = os.getenv("APIFY_TOKEN", "")
        if not token:
            return []
        results = []
        for name, url in ApifyCrawler.SITES:
            try:
                resp = requests.post(
                    f"https://api.apify.com/v2/acts/apify~cheerio-scraper/runs?token={token}",
                    json={
                        "startUrls": [{"url": url}],
                        "maxRequestsPerCrawl": 20,
                        "pageFunction": """async function pageFunction(context) {
                            const $ = context.jQuery;
                            const results = [];
                            $('a').each((i, el) => {
                                const title = $(el).text().trim();
                                const href = $(el).attr('href') || '';
                                if (title.length > 10 && title.length < 80) {
                                    results.push({ title, url: href });
                                }
                            });
                            return results.slice(0, 10);
                        }""",
                    },
                    timeout=30,
                )
                if resp.status_code == 201:
                    run_id = resp.json().get("data", {}).get("id", "")
                    for _ in range(6):
                        time.sleep(5)
                        status_resp = requests.get(
                            f"https://api.apify.com/v2/actor-runs/{run_id}/dataset/items?token={token}",
                            timeout=10,
                        )
                        if status_resp.status_code == 200:
                            items = status_resp.json()
                            for i, item in enumerate(items[:10]):
                                if isinstance(item, dict) and "title" in item:
                                    results.append({
                                        "keyword": item["title"],
                                        "source": f"apify_{name}",
                                        "score": (10 - i) * 4000,
                                    })
                            break
                print(f"  [OK] APIFY {name}: {len([r for r in results if name in r.get('source', '')])}ê°œ")
            except Exception as e:
                print(f"  [WARN] APIFY {name} ì‹¤íŒ¨: {e}")
        return results


# ============================================================================
# ì„ì‹œ íŒŒì¼ ì •ë¦¬
# ============================================================================
def cleanup_temp_files(work_dir: Path):
    temp_exts = {".ass", ".json"}
    removed = 0
    for f in work_dir.iterdir():
        if f.is_file() and f.suffix in temp_exts:
            if f.name == "script.json":
                continue
            try:
                f.unlink()
                removed += 1
            except OSError as e:
                logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
    if removed:
        print(f"  [OK] ì„ì‹œ íŒŒì¼ {removed}ê°œ ì •ë¦¬")


# ============================================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸ - ì˜ìƒ 1ê°œ ì™„ë²½ ìƒì„±
# ============================================================================
def make_one_perfect_short(
    upload: bool = False,
    scheduled: bool = False,
    video_index: int = 0,
    keep_temp: bool = True,
    topic: Optional[str] = None,
):
    """
    youshorts ì˜¬ì¸ì› ì˜ìƒ 1ê°œ ìƒì„± íŒŒì´í”„ë¼ì¸
    1â†’1.5â†’2â†’3â†’3.5â†’4â†’5â†’6â†’7â†’8(ì„ íƒ)
    """
    start_time = time.time()

    print("\n" + "=" * 60)
    print("  youshorts - ì™„ë²½í•œ ì˜ìƒ 1ê°œ ìƒì„± ì‹œì‘")
    print("=" * 60)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    work_dir = Config.OUTPUT_DIR / f"session_{timestamp}"
    work_dir.mkdir(parents=True, exist_ok=True)

    # â”€â”€ STEP 1: ì£¼ì œ ì„ ì • â”€â”€
    history = HistoryManager()

    if topic:
        selected_topic = topic
        selected_trend = {"keyword": topic, "source": "manual"}
        print(f"\n  ìˆ˜ë™ ì§€ì • ì£¼ì œ: {selected_topic}")
    else:
        trend_collector = TrendCollector()
        trends = trend_collector.collect_all()

        if not trends:
            print("\n  [WARN] íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ -> í´ë°± ì£¼ì œ")
            fallback_topics = [
                "ìš”ì¦˜ ê°€ì¥ í•«í•œ ìœ íŠœë¸Œ ë°ˆ ì´ì •ë¦¬",
                "ì—­ëŒ€ê¸‰ ë°˜ì „ ìˆëŠ” ì˜í™” 3í¸",
                "ì™¸êµ­ì¸ì´ í•œêµ­ ì™€ì„œ ì¶©ê²©ë°›ì€ ê²ƒë“¤",
                "í•œêµ­ì—ì„œë§Œ ê°€ëŠ¥í•œ ê²ƒë“¤ TOP5",
                "20ëŒ€ê°€ ëª¨ë¥´ë©´ ì†í•´ì¸ ì•± ì¶”ì²œ",
                "ì§ì¥ì¸ í‡´ê·¼ í›„ ë£¨í‹´ í˜„ì‹¤",
            ]
            trends = [{"keyword": random.choice(fallback_topics), "score": 0}]

        selected_trend = None
        for t in trends:
            if not history.is_duplicate(t["keyword"]):
                selected_trend = t
                break
        if not selected_trend:
            selected_trend = trends[0]

        selected_topic = selected_trend["keyword"]
        print(f"\n  ì„ ì •ëœ ì£¼ì œ: {selected_topic}")

    # â”€â”€ STEP 1.5: ë³¸ë¬¸ í¬ë¡¤ë§ + ë‰´ìŠ¤ ë³´ê°• â”€â”€
    source_text = ""
    post_url = selected_trend.get("url", "") if isinstance(selected_trend, dict) else ""
    if post_url and "community_" in str(selected_trend.get("source", "")):
        print(f"\n  ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œë„: {post_url[:80]}...")
        if not topic:  # trend_collector ìˆì„ ë•Œë§Œ
            source_text = trend_collector.fetch_post_body(post_url)
        if source_text:
            print(f"  [OK] ë³¸ë¬¸ ìˆ˜ì§‘: {len(source_text)}ì")
        else:
            print("  [WARN] ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨ â€” ë‰´ìŠ¤ë¡œ ë³´ê°•")

    if not source_text:
        news_collector = NewsCollector()
        news = news_collector.collect_news(selected_topic)
        if news:
            source_text = "\n".join(
                [f"- {n['title']}: {n.get('desc', '')}" for n in news[:5]]
            )

    # â”€â”€ STEP 2: ëŒ€ë³¸ ìƒì„± â”€â”€
    script_gen = ScriptGenerator()
    script_data = script_gen.generate(selected_topic, source_text)

    script_file = work_dir / "script.json"
    script_file.write_text(
        json.dumps(script_data, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    # â”€â”€ STEP 3: TTS ìƒì„± â”€â”€
    tts_engine = TTSEngine()
    tts_mp3 = str(work_dir / "tts.mp3")
    word_timings = tts_engine.generate(script_data["tts_script"], tts_mp3)

    timing_file = work_dir / "word_timings.json"
    timing_file.write_text(
        json.dumps(word_timings, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    # â”€â”€ STEP 3.5: ì˜¤ë””ì˜¤ ë§ˆìŠ¤í„°ë§ â”€â”€
    print("\n" + "=" * 60)
    print("STEP 3.5: ì˜¤ë””ì˜¤ ë§ˆìŠ¤í„°ë§ (-14 LUFS)")
    print("=" * 60)

    mastered_mp3 = str(work_dir / "tts_mastered.mp3")
    tts_mp3 = master_audio(tts_mp3, mastered_mp3)

    try:
        ffprobe = _find_ffprobe_exe()
        probe = subprocess.run(
            [ffprobe, "-v", "quiet", "-show_entries", "format=duration",
             "-of", "csv=p=0", tts_mp3],
            capture_output=True, text=True, timeout=10,
        )
        try:
            tts_duration = float(probe.stdout.strip())
        except (ValueError, TypeError):
            tts_duration = 40.0
        if tts_duration > Config.MAX_DURATION:
            speed = tts_duration / Config.MAX_DURATION
            adjusted_mp3 = str(work_dir / "tts_adjusted.mp3")
            tts_mp3 = adjust_audio_speed(tts_mp3, speed, adjusted_mp3)
    except (ValueError, subprocess.SubprocessError, OSError) as e:
        logger.warning(f"TTS ë“€ë ˆì´ì…˜ í™•ì¸ ì‹¤íŒ¨: {e}")
        tts_duration = 40.0

    # â”€â”€ STEP 4: ìë§‰ ìƒì„± â”€â”€
    print("\n" + "=" * 60)
    print("STEP 4: ë‹¨ì–´ë³„ í•˜ì´ë¼ì´íŠ¸ ìë§‰ (ASS)")
    print("=" * 60)

    subtitle_gen = SubtitleGenerator()
    ass_file = str(work_dir / "subtitles.ass")

    if word_timings:
        subtitle_gen.generate_ass(word_timings, ass_file)
    else:
        _dur = 40.0
        try:
            _r = VideoRenderer()
            _dur = _r._get_video_duration(tts_mp3)
        except Exception as e:
            logger.warning(f"ë“€ë ˆì´ì…˜ í™•ì¸ ì‹¤íŒ¨: {e}")
        subtitle_gen.generate_ass_from_chunks(
            script_data["tts_script"], _dur, ass_file
        )

    # â”€â”€ STEP 5: ë Œë”ë§ â”€â”€
    safe_title = re.sub(r'[\\/*?:"<>|()!\[\]{}]', '', script_data["title"])
    safe_title = safe_title.replace(" ", "_")[:30]
    output_filename = f"shorts_{safe_title}_{timestamp}.mp4"
    output_mp4 = str(work_dir / output_filename)

    renderer = VideoRenderer()
    final_video = renderer.render(
        tts_mp3, ass_file, output_mp4,
        topic=selected_topic, title=script_data.get("title", ""),
    )

    # â”€â”€ STEP 6: ì´ë ¥ ì €ì¥ â”€â”€
    print("\n" + "=" * 60)
    print("STEP 6: ì´ë ¥ ì €ì¥")
    print("=" * 60)
    history.save(selected_topic, script_data["title"], final_video)

    # â”€â”€ STEP 7: ë©”íƒ€ë°ì´í„° â”€â”€
    metadata = MetadataGenerator.generate(script_data)

    # â”€â”€ STEP 8: YouTube ì—…ë¡œë“œ (ì„ íƒ) â”€â”€
    upload_url = None
    if upload:
        print("\n" + "=" * 60)
        mode = "ì˜ˆì•½ ì—…ë¡œë“œ" if scheduled else "ì¦‰ì‹œ ì—…ë¡œë“œ"
        print(f"STEP 8: YouTube {mode}")
        print("=" * 60)
        uploader = YouTubeUploader()
        if scheduled:
            upload_url = uploader.upload_scheduled(final_video, metadata, video_index)
        else:
            upload_url = uploader.upload(final_video, metadata)

    if not keep_temp:
        cleanup_temp_files(work_dir)

    elapsed = time.time() - start_time

    print("\n" + "=" * 60)
    print(f"  ì˜ìƒ ìƒì„± ì™„ë£Œ!")
    print(f"  íŒŒì¼: {final_video}")
    print(f"  ì œëª©: {script_data.get('title', 'Unknown')}")
    print(f"  í’ˆì§ˆ: {script_data.get('quality_score', 'N/A')}ì ")
    print(f"  ì†Œìš”ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"  íƒœê·¸: {', '.join(script_data.get('tags', []))}")
    if upload_url:
        print(f"  YouTube: {upload_url}")
    print("=" * 60)

    return {
        "video": final_video,
        "title": script_data["title"],
        "description": metadata.get("description", ""),
        "tags": metadata.get("tags", []),
        "hashtags": metadata.get("hashtags", []),
        "quality_score": script_data.get("quality_score", 0),
        "elapsed_seconds": elapsed,
        "youtube_url": upload_url,
    }


# ============================================================================
# ë°°ì¹˜ ìƒì‚° (--count N)
# ============================================================================
def batch_produce(
    count: int = 3, upload: bool = False,
    scheduled: bool = False, topic: Optional[str] = None,
):
    print("\n" + "=" * 60)
    print(f"  ë°°ì¹˜ ìƒì‚° ì‹œì‘: {count}ê°œ")
    print("=" * 60)

    results = []
    for i in range(count):
        if not check_daily_limit():
            print(f"\n  [STOP] ì¼ì¼ í•œë„ ë„ë‹¬ â€” {i}ê°œ ìƒì‚° í›„ ì¤‘ë‹¨")
            break

        print(f"\n{'=' * 60}")
        print(f"  [{i + 1}/{count}] ì˜ìƒ ìƒì‚° ì¤‘...")
        print(f"{'=' * 60}")

        try:
            result = make_one_perfect_short(
                upload=upload, scheduled=scheduled,
                video_index=i, keep_temp=False, topic=topic,
            )
            results.append(result)
            print(f"\n  [OK] [{i + 1}/{count}] ì™„ë£Œ: {result['title']}")

            if i < count - 1:
                cooldown = random.randint(10, 30)
                print(f"  {cooldown}ì´ˆ ì¿¨ë‹¤ìš´...")
                time.sleep(cooldown)

        except Exception as e:
            print(f"\n  [ERROR] [{i + 1}/{count}] ì‹¤íŒ¨: {e}")
            results.append({"error": str(e)})

    FileCleaner.clean_output()

    success = [r for r in results if "video" in r]
    failed = [r for r in results if "error" in r]

    print("\n" + "=" * 60)
    print(f"  ë°°ì¹˜ ìƒì‚° ì™„ë£Œ! ì„±ê³µ: {len(success)}ê°œ, ì‹¤íŒ¨: {len(failed)}ê°œ")
    for r in success:
        yt = r.get("youtube_url", "") or ""
        print(f"  - {r['title']} ({r.get('quality_score', '?')}ì ) {yt}")
    print("=" * 60)

    return results


# ============================================================================
# CLI ì¸í„°í˜ì´ìŠ¤
# ============================================================================
def main():
    parser = argparse.ArgumentParser(
        description="youshorts ì˜¬ì¸ì› ìˆì¸  ìƒì„±ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python perfect_one_shot.py                        # ì˜ìƒ 1ê°œ ìƒì„±
  python perfect_one_shot.py --count 3              # 3ê°œ ì—°ì† ìƒì‚°
  python perfect_one_shot.py --count 5 --upload     # 5ê°œ + YouTube ì—…ë¡œë“œ
  python perfect_one_shot.py --upload --scheduled   # ì˜ˆì•½ ì—…ë¡œë“œ (4ì‹œê°„ ê°„ê²©)
  python perfect_one_shot.py --topic "í¸ì˜ì  ê¿€ì¡°í•©" # ì£¼ì œ ì§ì ‘ ì§€ì •
  python perfect_one_shot.py --keep-temp            # ì„ì‹œíŒŒì¼ ë³´ê´€
        """,
    )
    parser.add_argument("--count", "-n", type=int, default=1, help="ì˜ìƒ ê°œìˆ˜")
    parser.add_argument("--topic", "-t", type=str, default=None, help="ì£¼ì œ ì§ì ‘ ì§€ì •")
    parser.add_argument("--upload", "-u", action="store_true", help="YouTube ì—…ë¡œë“œ")
    parser.add_argument("--scheduled", "-s", action="store_true", help="ì˜ˆì•½ ì—…ë¡œë“œ")
    parser.add_argument("--keep-temp", action="store_true", help="ì„ì‹œíŒŒì¼ ë³´ê´€")
    parser.add_argument("--no-clean", action="store_true", help="ì •ë¦¬ ìŠ¤í‚µ")
    parser.add_argument("--max-daily", type=int, default=None, help="ì¼ì¼ í•œë„")
    args = parser.parse_args()

    if args.max_daily is not None:
        Config.MAX_PER_DAY = args.max_daily

    if args.count > 1:
        batch_produce(
            count=args.count, upload=args.upload,
            scheduled=args.scheduled, topic=args.topic,
        )
    else:
        make_one_perfect_short(
            upload=args.upload, scheduled=args.scheduled,
            keep_temp=args.keep_temp, topic=args.topic,
        )


if __name__ == "__main__":
    main()
